<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Mathematics, Python, ML, Data">
<meta property="og:type" content="website">
<meta property="og:title" content="Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Mathematics, Python, ML, Data">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Dongyoung Kim">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">공부한 내용을 정리하는 장소</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">10</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/03/nlp/from-scratch/ch08_Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/03/nlp/from-scratch/ch08_Attention/" class="post-title-link" itemprop="url">Ch 08. Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-11-03T00:00:00+09:00">2021-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-11 17:46:04" itemprop="dateModified" datetime="2021-11-11T17:46:04+09:00">2021-11-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><h3 id="seq2seq의-문제점"><a href="#seq2seq의-문제점" class="headerlink" title="seq2seq의 문제점"></a>seq2seq의 문제점</h3><ul>
<li>seq2seq은 Encoder가 문장을 ‘고정 길이’의 벡터로 변환해서 Decoder에 전달함.<br>문장 길이와 관계 없이 ‘고정 길이’로 변환하기 때문에 긴 문장의 경우 필요한 정보가 다 담기지 못하는 문제가 발생함.</li>
</ul>
<h3 id="Endoder-개선"><a href="#Endoder-개선" class="headerlink" title="Endoder 개선"></a>Endoder 개선</h3><ul>
<li>개선 방법으로, 각 단어 (시각)별 hidden state $h$를 모두 출력($hs$)<br><img src="../image/nlp-from-scratch/fig%208-2.png" alt="8-2"></li>
<li>이 떄, 각 시각의 $h$에는 직전에 입력된 단어(Encoder의 input)에 대한 정보가 많이 포함되어 있음.</li>
</ul>
<h3 id="Decoder-개선"><a href="#Decoder-개선" class="headerlink" title="Decoder 개선"></a>Decoder 개선</h3><ul>
<li>Encoder가 각 단어에 대응하는 LSTM layer의 hidden state $h$를 모아서 $hs$로 출력해서 Decoder에 전달하면, Decoder는 $hs$를 다시 문장으로 변환함.<br><img src="../image/nlp-from-scratch/fig%208-4.png" alt="8-4"></li>
<li>이 과정에서 ‘입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가?’(alignment)를 학습시키고자 함.<br>‘도착어 단어’(I)와 대응 관계에 있는 ‘출발어 단어’(나)의 정보를 골라내기 위해 Attention 매커니즘을 사용</li>
</ul>
<h4 id="Weighted-Sum"><a href="#Weighted-Sum" class="headerlink" title="Weighted Sum"></a>Weighted Sum</h4><ul>
<li>Decoder의 각 시각마다, 입력된 단어와 대응 관계인 단어의 벡터를 $hs$에서 골라내고자 함.<br>그러나 $hs$에서 벡터를 선택하는 작업은 미분할 수 없기 때문에 back-propagation을 위해 미분 가능한 연산으로 대체</li>
<li>각 단어의 중요도를 나타내는 가중치 $a$와 각 단어의 벡터 $hs$로부터 weighted sum을 구하여 맥락 벡터 $c$를 얻음<br><img src="../image/nlp-from-scratch/fig%208-8.png" alt="8-8"></li>
<li>$c$에 현 시각의 변환을 수행하는데 필요한 정보(단어간의 대응 관계, alignment)가 담기는 방향으로 학습</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeightSum</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hs, a</span>):</span></span><br><span class="line">        N, T, H = hs.shape</span><br><span class="line">        </span><br><span class="line">        ar = a.reshape(N, T, <span class="number">1</span>).repeat(H, axis=<span class="number">2</span>)</span><br><span class="line">        t = hs * ar</span><br><span class="line">        c = np.<span class="built_in">sum</span>(t, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.cache = (hs, ar)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> c</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dc</span>):</span></span><br><span class="line">        hs, ar = self.cache</span><br><span class="line">        N, T, H = hs.shape</span><br><span class="line">        </span><br><span class="line">        dt = dc.reshape(N, <span class="number">1</span>, H).repeat(T, axis=<span class="number">1</span>)</span><br><span class="line">        dar = dt * hs</span><br><span class="line">        dhs = dt * ar</span><br><span class="line">        da = np.<span class="built_in">sum</span>(dar, axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dhs, da</span><br></pre></td></tr></table></figure>
<h4 id="a를-구하는-방법"><a href="#a를-구하는-방법" class="headerlink" title="a를 구하는 방법"></a>a를 구하는 방법</h4><ul>
<li>Decoder의 LSTM layer의 $h$가 Encoder의 출력 $hs$의 각 단어 벡터와 얼마나 ‘비슷한지’를 수치로 나타내서 가중치로 이용.  </li>
<li>$h$와 $hs$의 내적을 통해 유사도 점수 $s$를 구하고, Softmax를 통해 정규화해서 가중치 $a$를 구함.<br><img src="../image/nlp-from-scratch/fig%208-15.png" alt="8-15"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&#x27;..&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> common.np <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.layers <span class="keyword">import</span> Softmax</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionWeight</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.softmax = Softmax()</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hs, h</span>):</span></span><br><span class="line">        N, T, H = hs.shape</span><br><span class="line">        </span><br><span class="line">        hr = h.reshape(N, <span class="number">1</span>, H).repeat(T, axis=<span class="number">1</span>)</span><br><span class="line">        t = hs * hr</span><br><span class="line">        s = np.<span class="built_in">sum</span>(t, axis=<span class="number">2</span>)</span><br><span class="line">        a = self.softmax.forward(s)</span><br><span class="line">        </span><br><span class="line">        self.cache = (hs, hr)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, da</span>):</span></span><br><span class="line">        hs, hr = self.cache</span><br><span class="line">        N, T, H = hs.shape</span><br><span class="line">        </span><br><span class="line">        ds = self.softmax.backward(da)</span><br><span class="line">        dt = ds.reshape(N, T, <span class="number">1</span>).repeat(H, axis=<span class="number">2</span>)</span><br><span class="line">        dhs = dt*hr</span><br><span class="line">        dhr = dt*hs</span><br><span class="line">        dh = np.<span class="built_in">sum</span>(dhr, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dhs, dh</span><br></pre></td></tr></table></figure>
<h4 id="결합"><a href="#결합" class="headerlink" title="결합"></a>결합</h4><ul>
<li>위에서 구현한 Weight Sum, Attention Weight를 하나로 결합.<br><img src="../image/nlp-from-scratch/fig%208-16.png" alt="8-16"></li>
<li>Attention Weight layer가 Encoder의 출력 $hs$에 주목하여 각 단어의 가중치 $a$를 구하고, $a$와 $hs$의 Weighted Sum을 통해 맥락 벡터 $c$를 구함</li>
<li>이 과정을 수행하는 layer를 Attention layer라고 부름.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>:</span></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">           self.params, self.grads = [], []</span><br><span class="line">           self.attention_weight_layer = AttentionWeight()</span><br><span class="line">           self.weight_sum_layer = WeightSum()</span><br><span class="line">           self.attention_weight = <span class="literal">None</span></span><br><span class="line">           </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hs, h</span>):</span></span><br><span class="line">           a = self.attention_weight_layer.forward(hs, h)</span><br><span class="line">           out = self.weight_sum_layer.forward(hs, a)</span><br><span class="line">           self.attention_weight = a</span><br><span class="line">           </span><br><span class="line">           <span class="keyword">return</span> out</span><br><span class="line">       </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">           dhs0, da = self.weight_sum_layer.backward(dout)</span><br><span class="line">           dhs1, dh = self.attention_weight_layer.backward(da)</span><br><span class="line">           dhs = dhs0 + dhs1</span><br><span class="line">           </span><br><span class="line">           <span class="keyword">return</span> dhs, dh      </span><br></pre></td></tr></table></figure>
<ul>
<li>앞선 layer들과 마찬가지로 Time Attention layer 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAttention</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line">        self.attention_weights = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hs_enc, hs_dec</span>):</span></span><br><span class="line">        N, T, H = hs_dec.shape</span><br><span class="line">        out = np.empty_like(hs_dec)</span><br><span class="line">        self.layers = []</span><br><span class="line">        self.attention_weights = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = Attention()</span><br><span class="line">            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">            self.attention_weights.append(layer.attention_weight)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        N, T, H = dout.shape</span><br><span class="line">        dhs_enc = <span class="number">0</span></span><br><span class="line">        dhs_dec = np.empty_like(dout)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dhs, dh = layer.backward(dout[:, t, :])</span><br><span class="line">            dhs_enc += dhs</span><br><span class="line">            dhs_dec[:, t, :] = dh</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dhs_enc, dhs_dec</span><br></pre></td></tr></table></figure>
<ul>
<li>기존 Affine layer와 LSTM layer 사이에 Attention layer를 넣어서 Decoder에 어텐션 정보를 추가</li>
</ul>
<h3 id="Attention을-포함한-seq2seq-구현"><a href="#Attention을-포함한-seq2seq-구현" class="headerlink" title="Attention을 포함한 seq2seq 구현"></a>Attention을 포함한 seq2seq 구현</h3><ul>
<li>앞에서 구현해놓은 Enccoder에서 LSTM layer의 마지막 $h$만 반환하던 것에서 모든 $h$를 반환하는 것으로 변경</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sys.path.append(<span class="string">&#x27;../code/seq2seq&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> seq2seq <span class="keyword">import</span> Encoder, Seq2seq</span><br><span class="line"><span class="keyword">from</span> common.time_layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionEncoder</span>(<span class="params">Encoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        xs = self.embed.forward(xs)</span><br><span class="line">        hs = self.lstm.forward(xs)</span><br><span class="line">        <span class="keyword">return</span> hs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dhs</span>):</span></span><br><span class="line">        dout = self.lstm.backward(dhs)</span><br><span class="line">        dout = self.embed.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br></pre></td></tr></table></figure>
<ul>
<li>Decoder의 경우 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionDecoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span>*H)/np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span>*H)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span>*H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(<span class="number">2</span>*H, V)/np.sqrt(<span class="number">2</span>*H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># attention layer 추가</span></span><br><span class="line">        self.attention = TimeAttention()</span><br><span class="line">        self.affine = TimeAffine(affine_W, affine_b)</span><br><span class="line">        layers = [self.embed, self.lstm, self.attention, self.affine]</span><br><span class="line">        </span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, enc_hs</span>):</span></span><br><span class="line">        h = enc_hs[:,-<span class="number">1</span>]</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        out = self.embed.forward(xs)</span><br><span class="line">        dec_hs = self.lstm.forward(out)</span><br><span class="line">        <span class="comment"># attention layer를 통해 c 계산</span></span><br><span class="line">        c = self.attention.forward(enc_hs, dec_hs)</span><br><span class="line">        out = np.concatenate((c, dec_hs), axis=<span class="number">2</span>)</span><br><span class="line">        score = self.affine.forward(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dscore</span>):</span></span><br><span class="line">        dout = self.affine.backward(dscore)</span><br><span class="line">        N, T, H2 = dout.shape</span><br><span class="line">        H = H2 // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]</span><br><span class="line">        denc_hs, ddec_hs1 = self.attention.backward(dc)</span><br><span class="line">        ddec_hs = ddec_hs0 + ddec_hs1</span><br><span class="line">        dout = self.lstm.backward(ddec_hs)</span><br><span class="line">        dh = self.lstm.dh</span><br><span class="line">        denc_hs[:, -<span class="number">1</span>] += dh</span><br><span class="line">        self.embed.backward(dout)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> denc_hs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, enc_hs, start_id, sample_size</span>):</span></span><br><span class="line">        sampled = []</span><br><span class="line">        sample_id = start_id</span><br><span class="line">        h = enc_hs[:, -<span class="number">1</span>]</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size):</span><br><span class="line">            x = np.array([sample_id]).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            out = self.embed.forward(x)</span><br><span class="line">            dec_hs = self.lstm.forward(out)</span><br><span class="line">            c = self.attention.forward(enc_hs, dec_hs)</span><br><span class="line">            out = np.concatenate((c, dec_hs), axis=<span class="number">2</span>)</span><br><span class="line">            score = self.affine.forward(out)</span><br><span class="line"></span><br><span class="line">            sample_id = np.argmax(score.flatten())</span><br><span class="line">            sampled.append(sample_id)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sampled</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>위 Attention Encoder와 Attention Decoder를 결합하여 Attention Seq2seq 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionSeq2seq</span>(<span class="params">Seq2seq</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        args = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        self.encoder = AttentionEncoder(*args)</span><br><span class="line">        self.decoder = AttentionDecoder(*args)</span><br><span class="line">        self.softmax = TimeSoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        self.params = self.encoder.params + self.decoder.params</span><br><span class="line">        self.grads = self.encoder.grads + self.decoder.grads</span><br></pre></td></tr></table></figure>
<h2 id="날짜-형식-변환-문제"><a href="#날짜-형식-변환-문제" class="headerlink" title="날짜 형식 변환 문제"></a>날짜 형식 변환 문제</h2><ul>
<li>날짜 형식 변환 문제를 통해 위에서 구현한 Attention seq2seq의 성능을 평가해 볼 것.<br><img src="../image/nlp-from-scratch/fig%208-22.png" alt="8-22"></li>
</ul>
<h3 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data load</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;date.txt&#x27;</span>)</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># reverse</span></span><br><span class="line">x_train, x_test = x_train[:, ::-<span class="number">1</span>], x_test[:, ::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameter 설정</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">10</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line">model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">acc_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, </span><br><span class="line">               batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        question, correct = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, question, correct, </span><br><span class="line">                                   id_to_char, verbose, is_reverse=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list.append(acc)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;val acc : <span class="subst">&#123;acc*<span class="number">100</span>&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 파라미터 저장</span></span><br><span class="line">model.save_params()</span><br></pre></td></tr></table></figure>
<h3 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h3><ul>
<li><p>첫 epoch에서 거의 다 틀리는 모습(accuracy 0%)</p>
<p>  | epoch 1 |  itr 1/351 | time 0.8640406131744385[s] | loss 4.07801399230957<br>  Q 10/15/94<br>  T 1994-10-15</p>
<h2 id="X-1978-08-11"><a href="#X-1978-08-11" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q thursday, november 13, 2008<br>  T 2008-11-13</p>
<h2 id="X-1978-08-11-1"><a href="#X-1978-08-11-1" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q Mar 25, 2003<br>  T 2003-03-25</p>
<h2 id="X-1978-08-11-2"><a href="#X-1978-08-11-2" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q Tuesday, November 22, 2016<br>  T 2016-11-22</p>
<h2 id="X-1978-08-11-3"><a href="#X-1978-08-11-3" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q Saturday, July 18, 1970<br>  T 1970-07-18</p>
<h2 id="X-1978-08-11-4"><a href="#X-1978-08-11-4" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q october 6, 1992<br>  T 1992-10-06</p>
<h2 id="X-1978-08-11-5"><a href="#X-1978-08-11-5" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q 8/23/08<br>  T 2008-08-23</p>
<h2 id="X-1978-08-11-6"><a href="#X-1978-08-11-6" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q 8/30/07<br>  T 2007-08-30</p>
<h2 id="X-1978-08-11-7"><a href="#X-1978-08-11-7" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q 10/28/13<br>  T 2013-10-28</p>
<h2 id="X-1978-08-11-8"><a href="#X-1978-08-11-8" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  Q sunday, november 6, 2016<br>  T 2016-11-06</p>
<h2 id="X-1978-08-11-9"><a href="#X-1978-08-11-9" class="headerlink" title="  X 1978-08-11"></a>  X 1978-08-11</h2><p>  val acc : 0.0</p>
</li>
<li><p>두번째 epoch에서 절반 정도 맞추기 시작(accuracy 51%)</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">| epoch 2 |  itr 1/351 | time 0.7569756507873535[s] | loss 1.0001761436462402</span><br><span class="line">Q 10/15/94                     </span><br><span class="line">T 1994-10-15</span><br><span class="line">O 1994-10-15</span><br><span class="line">---</span><br><span class="line">Q thursday, november 13, 2008  </span><br><span class="line">T 2008-11-13</span><br><span class="line">X 2006-11-13</span><br><span class="line">---</span><br><span class="line">Q Mar 25, 2003                 </span><br><span class="line">T 2003-03-25</span><br><span class="line">O 2003-03-25</span><br><span class="line">---</span><br><span class="line">Q Tuesday, November 22, 2016   </span><br><span class="line">T 2016-11-22</span><br><span class="line">O 2016-11-22</span><br><span class="line">---</span><br><span class="line">Q Saturday, July 18, 1970      </span><br><span class="line">T 1970-07-18</span><br><span class="line">O 1970-07-18</span><br><span class="line">---</span><br><span class="line">Q october 6, 1992              </span><br><span class="line">T 1992-10-06</span><br><span class="line">O 1992-10-06</span><br><span class="line">---</span><br><span class="line">Q 8/23/08                      </span><br><span class="line">T 2008-08-23</span><br><span class="line">O 2008-08-23</span><br><span class="line">---</span><br><span class="line">Q 8/30/07                      </span><br><span class="line">T 2007-08-30</span><br><span class="line">X 2007-08-09</span><br><span class="line">---</span><br><span class="line">Q 10/28/13                     </span><br><span class="line">T 2013-10-28</span><br><span class="line">X 1983-10-28</span><br><span class="line">---</span><br><span class="line">Q sunday, november 6, 2016     </span><br><span class="line">T 2016-11-06</span><br><span class="line">X 2016-11-08</span><br><span class="line">---</span><br><span class="line">val acc : 51.459999999999994</span><br></pre></td></tr></table></figure>
<ul>
<li>세번째 epoch부터는 대부분 맞추는 모습(accuracy 99.9%)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">| epoch 3 |  itr 1/351 | time 0.6751952171325684[s] | loss 0.3480678558349609</span><br><span class="line">Q 10/15/94                     </span><br><span class="line">T 1994-10-15</span><br><span class="line">O 1994-10-15</span><br><span class="line">---</span><br><span class="line">Q thursday, november 13, 2008  </span><br><span class="line">T 2008-11-13</span><br><span class="line">O 2008-11-13</span><br><span class="line">---</span><br><span class="line">Q Mar 25, 2003                 </span><br><span class="line">T 2003-03-25</span><br><span class="line">O 2003-03-25</span><br><span class="line">---</span><br><span class="line">Q Tuesday, November 22, 2016   </span><br><span class="line">T 2016-11-22</span><br><span class="line">O 2016-11-22</span><br><span class="line">---</span><br><span class="line">Q Saturday, July 18, 1970      </span><br><span class="line">T 1970-07-18</span><br><span class="line">O 1970-07-18</span><br><span class="line">---</span><br><span class="line">Q october 6, 1992              </span><br><span class="line">T 1992-10-06</span><br><span class="line">O 1992-10-06</span><br><span class="line">---</span><br><span class="line">Q 8/23/08                      </span><br><span class="line">T 2008-08-23</span><br><span class="line">O 2008-08-23</span><br><span class="line">---</span><br><span class="line">Q 8/30/07                      </span><br><span class="line">T 2007-08-30</span><br><span class="line">O 2007-08-30</span><br><span class="line">---</span><br><span class="line">Q 10/28/13                     </span><br><span class="line">T 2013-10-28</span><br><span class="line">O 2013-10-28</span><br><span class="line">---</span><br><span class="line">Q sunday, november 6, 2016     </span><br><span class="line">T 2016-11-06</span><br><span class="line">O 2016-11-06</span><br><span class="line">---</span><br><span class="line">val acc : 99.9</span><br><span class="line"></span><br><span class="line">| epoch 4 |  itr 1/351 | time 0.6612319946289062[s] | loss 0.0072284087538719176</span><br><span class="line">val acc : 99.9</span><br><span class="line">| epoch 5 |  itr 1/351 | time 0.7998595237731934[s] | loss 0.002182954549789429</span><br><span class="line">val acc : 99.92</span><br><span class="line">| epoch 6 |  itr 1/351 | time 0.6403138637542725[s] | loss 0.0011220933869481088</span><br><span class="line">val acc : 99.92</span><br><span class="line">| epoch 7 |  itr 1/351 | time 0.6831746101379395[s] | loss 0.0007817970588803291</span><br><span class="line">val acc : 99.92</span><br><span class="line">| epoch 8 |  itr 1/351 | time 0.7886159420013428[s] | loss 0.0004389991983771324</span><br><span class="line">val acc : 99.96000000000001</span><br><span class="line">| epoch 9 |  itr 1/351 | time 0.6941773891448975[s] | loss 0.0005494490731507539</span><br><span class="line">val acc : 99.96000000000001</span><br><span class="line">| epoch 10 |  itr 1/351 | time 1.4657552242279053[s] | loss 0.0002428537467494607</span><br><span class="line">val acc : 100.0</span><br></pre></td></tr></table></figure>
<ul>
<li>accuracy 시각화</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label=<span class="string">&#x27;attention&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%208-1.png" alt="out8-1"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/28/nlp/from-scratch/ch07_seq2seq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/28/nlp/from-scratch/ch07_seq2seq/" class="post-title-link" itemprop="url">Ch 07. seq2seq</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-28 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-28T00:00:00+09:00">2021-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-10 16:32:36" itemprop="dateModified" datetime="2021-11-10T16:32:36+09:00">2021-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Rnnlm-Generator"><a href="#Rnnlm-Generator" class="headerlink" title="Rnnlm Generator"></a>Rnnlm Generator</h2><ul>
<li>언어 모델(RNNLM)을 사용하여 문장 생성을 수행.</li>
<li>말뭉치를 사용해 학습한 언어 모델을 사용하여 새로운 문장을 만들어 냄</li>
<li>그 다음 개선된 언어 모델을 이용하여 더 자연스러운 문장을 생성</li>
<li>seq2seq : sequence(시계열 데이터)를 다른 sequence로 변환</li>
</ul>
<ul>
<li>deterministic :  특정 단어 뒤에 나올 확률이 가장 높은 단어를 선택</li>
<li>probabilistic : 확률적으로 다음 단어를 샘플링</li>
<li>위 과정을 종결 기호가 나타날때 까지 반복하여 새로운 문장 생성</li>
</ul>
<h4 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnlmGen</span>(<span class="params">Rnnlm</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, start_id, skip_ids=<span class="literal">None</span>, sample_size=<span class="number">100</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rnnlm을 이용하여 문장 생성.</span></span><br><span class="line"><span class="string">        start_id : 첫 단어의 ID</span></span><br><span class="line"><span class="string">        skip_ids : 샘플링되지 않을 단어를 지정해주는 리스트 </span></span><br><span class="line"><span class="string">        (예를 들어 PTB data set에서 &lt;unk&gt; : 희소한 단어, N : 숫자 등을 제외)</span></span><br><span class="line"><span class="string">        sample_size : 샘플링 할 단어의 수</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        word_ids = [start_id]</span><br><span class="line">        </span><br><span class="line">        x = start_id</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(word_ids) &lt; sample_size:</span><br><span class="line">            x = np.array(x).reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">            score = self.predict(x)</span><br><span class="line">            p = softmax(score.flatten())</span><br><span class="line">            </span><br><span class="line">            sampled = np.random.choice(<span class="built_in">len</span>(p), size=<span class="number">1</span>, p=p)</span><br><span class="line">            <span class="keyword">if</span> (skip_ids <span class="keyword">is</span> <span class="literal">None</span>) <span class="keyword">or</span> (sampled <span class="keyword">not</span> <span class="keyword">in</span> skip_ids):</span><br><span class="line">                x = sampled</span><br><span class="line">                word_ids.append(<span class="built_in">int</span>(x))</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> word_ids</span><br></pre></td></tr></table></figure>
<h4 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h4><ul>
<li>위 구현을 사용하여 ‘you’로 시작하는 문장 생성해보기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">corpus_size = <span class="built_in">len</span>(corpus)</span><br><span class="line"></span><br><span class="line">model = RnnlmGen()</span><br><span class="line"><span class="comment"># model.load_params(../practice/Rnnlm.pkl)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><pre><code>you lenses beings wishes donoghue violin hourly listen manage chair trinova bergsma reopen essentially ann fan phased e two-year middlemen darman retail dealers open running quota managed hill stem lighting claims incorporated professor grid tough nor honecker apparently livestock wellington broker-dealer bobby shareholders guterman willful old-fashioned such heels does fearful donald dominates which assembly very genetically region packwood establishing divisive suburb slow am cars gatt draws unlawful reacting emerging technologies animals johnson applied treat hurdle network beleaguered reformers commonwealth hurdles transactions broderick which unesco watch block careful builds walked prevailed grew pittsburgh responsibility bets suits highlight slip liquidate exterior experts
</code></pre><h4 id="6장에서-학습시켰던-모델을-사용해-문장-생성"><a href="#6장에서-학습시켰던-모델을-사용해-문장-생성" class="headerlink" title="6장에서 학습시켰던 모델을 사용해 문장 생성"></a>6장에서 학습시켰던 모델을 사용해 문장 생성</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model = RnnlmGen()</span></span><br><span class="line">model.load_params(<span class="string">&#x27;../practice/Rnnlm.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<h4 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h4><pre><code>you retain evaluation such bad municipal managers and constitute the written mood.
 he is to be at john salinas calif. resigned.
 mr. roman was among a share of mr. simmons and disclosed the federal court threw by the fund because they would n&#39;t retreated the agreement to assist its two uv-b advice.
 joining these reviews resources on county new jersey comparison that warner was out it too.
 the west german news is increasingly buying seriously to bankers and lincoln soon to make up their own troubles.
 mr. stone canada the chapter year lacks he had
</code></pre><h3 id="Better-RnnlmGen"><a href="#Better-RnnlmGen" class="headerlink" title="Better RnnlmGen"></a>Better RnnlmGen</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BetterRnnlmGen</span>(<span class="params">BetterRnnlm</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, start_id, skip_ids=<span class="literal">None</span>, sample_size=<span class="number">100</span></span>):</span></span><br><span class="line">        word_ids = [start_id]</span><br><span class="line"></span><br><span class="line">        x = start_id</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(word_ids) &lt; sample_size:</span><br><span class="line">            x = np.array(x).reshape(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            score = self.predict(x).flatten()</span><br><span class="line">            p = softmax(score).flatten()</span><br><span class="line"></span><br><span class="line">            sampled = np.random.choice(<span class="built_in">len</span>(p), size=<span class="number">1</span>, p=p)</span><br><span class="line">            <span class="keyword">if</span> (skip_ids <span class="keyword">is</span> <span class="literal">None</span>) <span class="keyword">or</span> (sampled <span class="keyword">not</span> <span class="keyword">in</span> skip_ids):</span><br><span class="line">                x = sampled</span><br><span class="line">                word_ids.append(<span class="built_in">int</span>(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        states = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.lstm_layers:</span><br><span class="line">            states.append((layer.h, layer.c))</span><br><span class="line">        <span class="keyword">return</span> states</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span>(<span class="params">self, states</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer, state <span class="keyword">in</span> <span class="built_in">zip</span>(self.lstm_layers, states):</span><br><span class="line">            layer.set_state(*state)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlmGen()</span><br><span class="line"><span class="comment">#model.load_params(&#x27;../practice/Rnnlm.pkl&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>you modestly mead methods tenn. chlorofluorocarbons photographic cypress substance medicare consequences deny via modified mph welch followed spurred determined failures lawsuits poland w. making prepares machinists yields pakistan sugar assault impending editor unfavorable implied priority we purchase coupons flagship tree hang smiling bridge duck joe agreement lighter release deadlines or drivers checks expenses recall decides pipelines movies practiced equity exhibit financially lawsuit reality so-called lesser press automotive springs restoration workplace continually calendar ai fighter dealing hourly ivory arnold suitable mentality improved j.c. designing procedures trader foundations kerry sank strategic show reference fame chaotic competition very fha reading releases minor stretching
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlmGen()</span><br><span class="line">model.load_params(<span class="string">&#x27;../code/rnnlm/BetterRnnlm.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>you did n&#39;t know how a spokesman said in no matter.
 they was very skeptical of the lone star &#39;s line in union by a worth.
 for that time it will take payment of the large market for salespeople.
 a ministry official spent a hat regarding the new state record against a led to the national association of journal and financial services the olympic office of science which had begun a wall breaker palace and the orange workers.
 in his wake in june she cautioned the company &#39;s defense portfolio would be selling about the third
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model.reset_state()</span><br><span class="line"></span><br><span class="line">start_words = <span class="string">&#x27;the meaning of life is&#x27;</span></span><br><span class="line">start_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> start_words.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> start_ids[:-<span class="number">1</span>]:</span><br><span class="line">    x = np.array(x).reshape(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    model.predict(x)</span><br><span class="line"></span><br><span class="line">word_ids = model.generate(start_ids[-<span class="number">1</span>], skip_ids)</span><br><span class="line">word_ids = start_ids[:-<span class="number">1</span>] + word_ids</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>--------------------------------------------------
the meaning of life is a while it could harm the problem of the trigger to make those difficulties in an adversary treaty are such chemistry as the nation &#39;s defense opposition.
 the prosecutor would assume that if that gets business money would otherwise make it difficult for its future calculations.
 if people needed money like good research the need for a one-hour champion to finance techniques.
 white activists and consultants wo n&#39;t go good until most say they did n&#39;t know deals with a different class but they still are simply considering a death and carry sent big-time tax benefits
</code></pre><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><ul>
<li>Encoder - Decoder : LSTM 2개로 구성</li>
<li>Encoder가 문장을 hidden state $h$로 변환시키면</li>
<li>Decoder가 $h$를 입력으로 받아서 문장을 생성</li>
</ul>
<h3 id="toy-problem"><a href="#toy-problem" class="headerlink" title="toy problem"></a>toy problem</h3><ul>
<li><p>덧셈 문제</p>
<p>  <img src="../image/nlp-from-scratch/fig%207-10.png" alt="7-10"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&#x27;..&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> sequence</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>([id_to_char[c] <span class="keyword">for</span> c <span class="keyword">in</span> x_train[<span class="number">0</span>]], [id_to_char[c] <span class="keyword">for</span> c <span class="keyword">in</span> t_train[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;7&#39;, &#39;1&#39;, &#39;+&#39;, &#39;1&#39;, &#39;1&#39;, &#39;8&#39;, &#39; &#39;] [&#39;_&#39;, &#39;1&#39;, &#39;8&#39;, &#39;9&#39;, &#39; &#39;]
</code></pre><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>문자열을 입력받아서 벡터 $h$로 변환</li>
<li>LSTM을 사용하는 경우 hidden state $h$만 Decoder로 전달.<br>  cell $c$는 LSTM 자기 자신만 사용한다는 전제로 만들어졌기 때문.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        vocab_size : 어휘 수</span></span><br><span class="line"><span class="string">        wordvec_size : word vector의 dimension</span></span><br><span class="line"><span class="string">        hidden_size : LSTM layer의 hidden state vector의 dimension</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span>*H)/np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span>*H)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span>*H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.params = self.embed.params + self.lstm.params</span><br><span class="line">        self.grads = self.embed.grads + self.lstm.grads</span><br><span class="line">        self.hs = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        xs = self.embed.forward(xs)</span><br><span class="line">        hs = self.lstm.forward(xs)</span><br><span class="line">        self.hs = hs</span><br><span class="line">        <span class="keyword">return</span> hs[:,-<span class="number">1</span>,:]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dh</span>):</span></span><br><span class="line">        dhs = np.zeros_like(self.hs)</span><br><span class="line">        dhs[:, -<span class="number">1</span>, :] = dh</span><br><span class="line">        </span><br><span class="line">        dout = self.lstm.backward(dhs)</span><br><span class="line">        dout = self.embed.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br></pre></td></tr></table></figure>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li><p>Encoder가 출력한 $h$를 입력받아 목적으로 하는 다른 문자열을 출력</p>
</li>
<li><p>RNN으로 문장을 생성할 때, 학습 시에는 정답을 알고 있기 때문에 sequence를 한번에 입력.<br>하지만 문장을 생성할 때는 시작을 알리는 구분문자(‘_’)를 입력하고 다음 출력 문자를 입력으로 반복</p>
</li>
<li><p>위 덧셈 문제의 경우 답이 정해져 있는 문제기 때문에 deterministic하게(점수가 가장 높은 문자 고르기) 문자열 생성해 봄</p>
<p><img src="../image/nlp-from-scratch/fig%207-19.png" alt="7-19"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span>*H)/np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span>*H)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span>*H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(H,V)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>)</span><br><span class="line">        self.affine = TimeAffine(affine_W, affine_b)</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> (self.embed, self.lstm, self.affine):</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, h</span>):</span></span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        out = self.embed.forward(xs)</span><br><span class="line">        out = self.lstm.forward(out)</span><br><span class="line">        score = self.affine.forward(out)</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dscore</span>):</span></span><br><span class="line">        dout = self.affine.backward(dscore)</span><br><span class="line">        dout = self.lstm.backward(dout)</span><br><span class="line">        dout = self.embed.backward(dout)</span><br><span class="line">        dh = self.lstm.dh</span><br><span class="line">        <span class="keyword">return</span> dh</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, h, start_id, sample_size</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        위 forward는 학습 시 사용</span></span><br><span class="line"><span class="string">        generate는 새 문장을 생성할 때 사용</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        sampled = []</span><br><span class="line">        sample_id = start_id</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size):</span><br><span class="line">            x = np.array(sample_id).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            out = self.embed.forward(x)</span><br><span class="line">            out = self.lstm.forward(out)</span><br><span class="line">            score = self.affine.forward(out)</span><br><span class="line">            </span><br><span class="line">            sample_id = np.argmax(score.flatten())</span><br><span class="line">            sampled.append(<span class="built_in">int</span>(sample_id))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> sampled</span><br></pre></td></tr></table></figure>
<h3 id="Seq2seq-1"><a href="#Seq2seq-1" class="headerlink" title="Seq2seq"></a>Seq2seq</h3><ul>
<li>Encoder와 Decoder를 연결 후 Time Softmax with Loss를 통해 Loss 계산</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2seq</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        self.encoder = Encoder(V, D, H)</span><br><span class="line">        self.decoder = Decoder(V, D, H)</span><br><span class="line">        self.softmax = TimeSoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        self.params = self.encoder.params + self.decoder.params</span><br><span class="line">        self.grads = self.encoder.grads + self.decoder.grads</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        decoder_xs, decoder_ts = ts[:, :-<span class="number">1</span>], ts[:, <span class="number">1</span>:]</span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        score = self.decoder.forward(decoder_xs, h)</span><br><span class="line">        loss = self.softmax.forward(score, decoder_ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.softmax.backward(dout)</span><br><span class="line">        dh = self.decoder.backward(dout)</span><br><span class="line">        dout = self.encoder.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, xs, start_id, sample_size</span>):</span></span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        sampled = self.decoder.generate(h, start_id, sample_size)</span><br><span class="line">        <span class="keyword">return</span> sampled</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="덧셈-문제"><a href="#덧셈-문제" class="headerlink" title="덧셈 문제"></a>덧셈 문제</h3><ol>
<li>train data에서 mini-batch 선택</li>
<li>기울기 계산</li>
<li>parameter 갱신</li>
</ol>
<h4 id="Seq2seq-Evaluation-구현"><a href="#Seq2seq-Evaluation-구현" class="headerlink" title="Seq2seq Evaluation 구현"></a>Seq2seq Evaluation 구현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_seq2seq</span>(<span class="params">model, question, correct, id_to_char,</span></span></span><br><span class="line"><span class="params"><span class="function">                 verbos=<span class="literal">False</span>, is_reverse=<span class="literal">False</span></span>):</span></span><br><span class="line">    correct = correct.flatten()</span><br><span class="line">    <span class="comment"># 머릿글자</span></span><br><span class="line">    start_id = correct[<span class="number">0</span>]</span><br><span class="line">    correct = correct[<span class="number">1</span>:]</span><br><span class="line">    guess = model.generate(question, start_id, <span class="built_in">len</span>(correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 문자열로 변환</span></span><br><span class="line">    question = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> question.flatten()])</span><br><span class="line">    correct = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> correct])</span><br><span class="line">    guess = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> guess])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbos:</span><br><span class="line">        <span class="keyword">if</span> is_reverse:</span><br><span class="line">            question = question[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        colors = &#123;<span class="string">&#x27;ok&#x27;</span>: <span class="string">&#x27;\033[92m&#x27;</span>, <span class="string">&#x27;fail&#x27;</span>: <span class="string">&#x27;\033[91m&#x27;</span>, <span class="string">&#x27;close&#x27;</span>: <span class="string">&#x27;\033[0m&#x27;</span>&#125;</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Q&#x27;</span>, question)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;T&#x27;</span>, correct)</span><br><span class="line"></span><br><span class="line">        is_windows = os.name == <span class="string">&#x27;nt&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> correct == guess:</span><br><span class="line">            mark = colors[<span class="string">&#x27;ok&#x27;</span>] + <span class="string">&#x27;☑&#x27;</span> + colors[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> is_windows:</span><br><span class="line">                mark = <span class="string">&#x27;O&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(mark + <span class="string">&#x27; &#x27;</span> + guess)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark = colors[<span class="string">&#x27;fail&#x27;</span>] + <span class="string">&#x27;☒&#x27;</span> + colors[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> is_windows:</span><br><span class="line">                mark = <span class="string">&#x27;X&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(mark + <span class="string">&#x27; &#x27;</span> + guess)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> guess == correct <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h4 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line">model = Seq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-1.png" alt="out1"></p>
<h2 id="seq2seq-개선"><a href="#seq2seq-개선" class="headerlink" title="seq2seq 개선"></a>seq2seq 개선</h2><h3 id="Reverse"><a href="#Reverse" class="headerlink" title="Reverse"></a>Reverse</h3><ul>
<li>입력 데이터의 순서를 반전 시키면 학습 진행속도도 빨라지고, 정확도도 좋아짐<br><img src="../image/nlp-from-scratch/fig%207-23.png" alt="7-23"> </li>
<li>직관적으로 ‘나는 고양이로소이다’를 ‘I am a cat’로 번역하는 문제에서 back-propagation시<br>[‘이다’, ‘로소’, ‘고양이’, ‘는’, ‘나’]로 반전시키면 ‘나’와 ‘I’가 붙어있어서 기울기가 잘 전달 됨</li>
<li>입력 데이터를 반전시켜도 단어 사이의 ‘평균 거리’는 그대로임.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">x_train, x_test = x_train[:, ::-<span class="number">1</span>], x_test[:, ::-<span class="number">1</span>]</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = Seq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_rev = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_rev.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-2.png" alt="out2"></p>
<h3 id="Peeky-Decoder"><a href="#Peeky-Decoder" class="headerlink" title="Peeky Decoder"></a>Peeky Decoder</h3><ul>
<li>기존 seq2seq에서는 ‘입력 문장’을 Encoder가 $h$로 변환하여 Decoder에 전달하고<br>Decoder의 최초 시점의 LSTM layer에서만 $h$를 이용함</li>
<li>중요 정보인 $h$를 Decoder의 여러 layer에서 사용하는 것이 Peeky Decoder<br><img src="../image/nlp-from-scratch/fig%207-26.png" alt="7-26"></li>
<li>Decoder의 모든 시각($t$)의 Affine, LSTM layer에 $h$를 전달<br>여러 layer가 중요 정보인 $h$를 공유함으로서 더 올바른 결정을 내림</li>
<li>$h$에 대한 계산을 추가로 해줘야 하므로 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeekyDecoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        <span class="comment"># hidden 추가</span></span><br><span class="line">        lstm_Wx = (rn(H + D, <span class="number">4</span> * H) / np.sqrt(H + D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        <span class="comment">#hidden 추가</span></span><br><span class="line">        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>)</span><br><span class="line">        self.affine = TimeAffine(affine_W, affine_b)</span><br><span class="line">        </span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> (self.embed, self.lstm, self.affine):</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, h</span>):</span></span><br><span class="line">        N, T = xs.shape</span><br><span class="line">        N, H = h.shape</span><br><span class="line">        </span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        out = self.embed.forward(xs)</span><br><span class="line">        <span class="comment"># 추가된 hidden h와 기존에 전달되던 out을 concat</span></span><br><span class="line">        hs = np.repeat(h, T, axis=<span class="number">0</span>).reshape(N, T, H)</span><br><span class="line">        out = np.concatenate((hs, out), axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        out = self.lstm.forward(out)</span><br><span class="line">        <span class="comment"># 위와 마찬가지로 h 추가</span></span><br><span class="line">        out = np.concatenate((hs, out), axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        score = self.affine.forward(out)</span><br><span class="line">        self.cache = H</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dscore</span>):</span></span><br><span class="line">        H = self.cache</span><br><span class="line"></span><br><span class="line">        dout = self.affine.backward(dscore)</span><br><span class="line">        <span class="comment"># 추가된 h 반영</span></span><br><span class="line">        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]</span><br><span class="line">        dout = self.lstm.backward(dout)</span><br><span class="line">        <span class="comment"># 추가된 h 반영2</span></span><br><span class="line">        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]</span><br><span class="line">        self.embed.backward(dembed)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 추가된 h 반영3</span></span><br><span class="line">        dhs = dhs0 + dhs1</span><br><span class="line">        dh = self.lstm.dh + np.<span class="built_in">sum</span>(dhs, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dh</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, h, start_id, sample_size</span>):</span></span><br><span class="line">        sampled = []</span><br><span class="line">        char_id = start_id</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line"></span><br><span class="line">        H = h.shape[<span class="number">1</span>]</span><br><span class="line">        peeky_h = h.reshape(<span class="number">1</span>, <span class="number">1</span>, H)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size):</span><br><span class="line">            x = np.array([char_id]).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            out = self.embed.forward(x)</span><br><span class="line"></span><br><span class="line">            out = np.concatenate((peeky_h, out), axis=<span class="number">2</span>)</span><br><span class="line">            out = self.lstm.forward(out)</span><br><span class="line">            out = np.concatenate((peeky_h, out), axis=<span class="number">2</span>)</span><br><span class="line">            score = self.affine.forward(out)</span><br><span class="line"></span><br><span class="line">            char_id = np.argmax(score.flatten())</span><br><span class="line">            sampled.append(char_id)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sampled        </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeekySeq2seq</span>(<span class="params">Seq2seq</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        self.encoder = Encoder(V, D, H)</span><br><span class="line">        self.decoder = PeekyDecoder(V, D, H)</span><br><span class="line">        self.softmax = TimeSoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        self.params = self.encoder.params + self.decoder.params</span><br><span class="line">        self.grads = self.encoder.grads + self.decoder.grads</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        decoder_xs, decoder_ts = ts[:, :-<span class="number">1</span>], ts[:, <span class="number">1</span>:]</span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        score = self.decoder.forward(decoder_xs, h)</span><br><span class="line">        loss = self.softmax.forward(score, decoder_ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.softmax.backward(dout)</span><br><span class="line">        dh = self.decoder.backward(dout)</span><br><span class="line">        dout = self.encoder.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, xs, start_id, sample_size</span>):</span></span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        sampled = self.decoder.generate(h, start_id, sample_size)</span><br><span class="line">        <span class="keyword">return</span> sampled</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="Peeky-seq2seq-학습"><a href="#Peeky-seq2seq-학습" class="headerlink" title="Peeky seq2seq 학습"></a>Peeky seq2seq 학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line"><span class="comment">#x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]</span></span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_peeky = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_peeky.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-3.png" alt="out3"></p>
<h3 id="Peeky-seq2seq-reversed-학습"><a href="#Peeky-seq2seq-reversed-학습" class="headerlink" title="Peeky seq2seq(+ reversed) 학습"></a>Peeky seq2seq(+ reversed) 학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">x_train, x_test = x_train[:, ::-<span class="number">1</span>], x_test[:, ::-<span class="number">1</span>]</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_peeky_rev = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_peeky_rev.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky+reversed&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-4.png" alt="out4"></p>
<h2 id="seq2seq을-이용하는-애플리케이션"><a href="#seq2seq을-이용하는-애플리케이션" class="headerlink" title="seq2seq을 이용하는 애플리케이션"></a>seq2seq을 이용하는 애플리케이션</h2><p>seq2seq은 ‘한 시계열 데이터’를 ‘다른 시계열 데이터’로 변환하는 것</p>
<ul>
<li>기계 번역 : ‘한 언어의 문장’을 ‘다른 언어의 문장’으로 변환</li>
<li>자동 요약 : ‘긴 문장’을 ‘짧게 요약한 문장’으로 변환</li>
<li>질의 응답 : ‘질문’을 ‘응답’으로 변환</li>
<li>메일 자동 응답 : ‘받은 메일의 문장’을 ‘답변 글’로 변환</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/24/nlp/from-scratch/ch06_LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/24/nlp/from-scratch/ch06_LSTM/" class="post-title-link" itemprop="url">Ch 06. LSTM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-24 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-24T00:00:00+09:00">2021-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-08 22:43:46" itemprop="dateModified" datetime="2021-11-08T22:43:46+09:00">2021-11-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="기존-RNN의-문제점"><a href="#기존-RNN의-문제점" class="headerlink" title="기존 RNN의 문제점"></a>기존 RNN의 문제점</h2><ul>
<li><p>시계열 데이터에서 멀리 떨어진, 장기(long term) 의존 관계를 잘 학습할 수 없음</p>
<ul>
<li><p>BPTT에서 기울기 소실/폭발이 일어나기 때문</p>
<p><img src="/image/nlp-from-scratch/fig%206-3.png" alt="6-3"></p>
</li>
</ul>
</li>
<li><p>‘?’에 들어가는 단어는 ‘Tom’<br>RNNLM이 위 질문에 답하기 위해서는 ‘Tom이 방에서 TV를 보고 있음’과 ‘Mary가 방에 들어옴’이라는 두 정보를 모두 기억해야 함.<br><img src="/image/nlp-from-scratch/fig%206-4.png" alt="6-4"><br>위 그림에서 빨간색 화살표를 따라 기울기를 전달하여 학습하는데, 단어간의 거리가 멀어지면(long-term) 결국 기울기가 작아지거나(소실), 커져서(폭발) 장기 의존 관계를 학습할 수 없게 됨.  </p>
</li>
<li><p>기울기 소실(vanishing gradient)이 일어나는 이유</p>
<ul>
<li>$y = tanh(x)$의 미분은 ${\partial y \over \partial x} = 1 - y^2$으로 역전파 과정에서 기울기가 tanh 노드를 지날때마다 작아짐</li>
</ul>
</li>
<li><p>기울기 폭발(exploding gradients)</p>
<ul>
<li>Matrix multiplication을 실행하면서 시간에 따라 </li>
<li>Wh의 singular value의 최댓값이 1보다 큰 경우 : 기울기가 exponentially 증가</li>
<li>1보다 작은 경우: 기울기가 exponentially 감소(이 경우 vanishing gradient)<br>[30] ‘On the difficulty of training recurrent neural networks’</li>
</ul>
</li>
</ul>
<h3 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h3><ul>
<li><p>기울기 폭발의 해결방법 중 하나.  </p>
<script type="math/tex; mode=display">if~~\| \hat g \| \geq theshold~:~~~ \hat g = {threshold \over \| \hat g \|} \hat g</script></li>
<li><p>기울기 g가 일정 값(threshold) 이상으로 커지면 기울기를 수정해 주는 것.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_grads</span>(<span class="params">grads, max_norm</span>):</span></span><br><span class="line">    total_norm = <span class="number">0</span></span><br><span class="line">    <span class="comment"># g hat 계산</span></span><br><span class="line">    <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">        total_norm += np.<span class="built_in">sum</span>(grad**<span class="number">2</span>)</span><br><span class="line">    total_norm = np.sqrt(total_norm)</span><br><span class="line"></span><br><span class="line">    rate = max_norm / (total_norm + <span class="number">1e-6</span>)</span><br><span class="line">    <span class="keyword">if</span> rate &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">            grad *= rate</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dW1 = np.random.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">10</span></span><br><span class="line">dW2 = np.random.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">10</span></span><br><span class="line">grads = [dW1, dW2]</span><br><span class="line">max_norm = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(grads)</span><br><span class="line">clip_grads(grads, max_norm)</span><br><span class="line"><span class="built_in">print</span>(grads)</span><br></pre></td></tr></table></figure>
<p>기울기가 일정 값(이 경우 5)를 넘어가면 잘 수정해주는 것을 볼 수 있음</p>
<pre><code>[array([[6.49144048, 2.78487283, 6.76254902],
       [5.90862817, 0.23981882, 5.58854088],
       [2.59252447, 4.15101197, 2.83525082]]), array([[6.93137918, 4.40453718, 1.56867738],
       [5.44649018, 7.80314765, 3.06363532],
       [2.21957884, 3.87971258, 9.3638365 ]])]
[array([[1.49503731, 0.64138134, 1.55747605],
       [1.36081038, 0.05523244, 1.28709139],
       [0.59708178, 0.95601551, 0.65298384]]), array([[1.59635916, 1.01440465, 0.36128056],
       [1.25437583, 1.79713531, 0.70558286],
       [0.51118903, 0.89353281, 2.15657603]])]
</code></pre><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>이제 vanishing gradient를 해결하기 위해서 ‘게이트가 추가된 RNN’을 도입할 것<br>대표적으로 LSTM, GRU가 있고, LSTM을 먼저 다룰 것임  </p>
<p><img src="/image/nlp-from-scratch/fig%206-15.png" alt="6-15"></p>
<ul>
<li>LSTM에는 기존 RNN과 다르게 기억 셀(memory cell) $c$ 라는 경로가 있음  </li>
<li><script type="math/tex">c_t</script> 에는 시각 $t$에서의 LSTM의 기억이 저장되어 있음<br>이 `$c_t$를 통해 $h_t=tanh(c_t)$를 계산  <h4 id="output-gate의-열림-상태"><a href="#output-gate의-열림-상태" class="headerlink" title="output gate의 열림 상태"></a>output gate의 열림 상태</h4></li>
</ul>
<script type="math/tex; mode=display">o = \sigma(x_{t} W_{x}^{(o)} + h_{t-1}W_{h}^{(o)} + b^{(o)})</script><ul>
<li>$tanh(c_t)$의 원소들에 대해 ‘그것이 다음 시각의 hidden state에 얼마나 중요한가’를 조정  <ul>
<li>여기서 $\sigma$(sigmoid)의 출력은 0~1로 데이터를 얼마만큼 통과시킬지를 정하는 비율  </li>
</ul>
</li>
</ul>
<h4 id="output-gate"><a href="#output-gate" class="headerlink" title="output gate"></a>output gate</h4><script type="math/tex; mode=display">h_{t} = o \circ tanh(c_{t})</script><p>여기서 tanh의 출력은 -1.0~1.0으로 ‘정보’의 강약(정도)를 표시한다고 볼 수 있음  </p>
<ul>
<li>정보를 얼마만큼 통과시킬지 정하는 sigmoid와 다르게 실질적인 ‘정보’를 지니는 데이터에는 tanh를 주로 사용</li>
</ul>
<h4 id="forget-gate"><a href="#forget-gate" class="headerlink" title="forget gate"></a>forget gate</h4><p><img src="/image/nlp-from-scratch/fig%206-16.png" alt="6-16"></p>
<ul>
<li><p>$c_{t-1}$의 기억 중 불필요한 기억을 잊게 해주는 게이트  </p>
<script type="math/tex; mode=display">f = \sigma(x_{t} W_{x}^{(f)} + h_{t-1}W_{h}^{(f)} + b^{(f)})</script><script type="math/tex; mode=display">c_{t} = f \circ c_{t-1}</script></li>
</ul>
<h4 id="새로운-기억-셀"><a href="#새로운-기억-셀" class="headerlink" title="새로운 기억 셀"></a>새로운 기억 셀</h4><p><img src="/image/nlp-from-scratch/fig%206-17.png" alt="6-17"></p>
<ul>
<li><p>새로 기억해야 할 정보를 기억 셀 $c$에 추가    </p>
<script type="math/tex; mode=display">g = \tanh(x_{t} W_{x}^{(g)} + h_{t-1}W_{h}^{(g)} + b^{(g)})</script></li>
<li><p>잊어버릴 정보가 아니라 실질적으로 ‘기억’해야할 정보기 때문에 $sigmoid$가 아닌 $tanh$ 사용</p>
<h4 id="input-gate"><a href="#input-gate" class="headerlink" title="input gate"></a>input gate</h4><p><img src="/image/nlp-from-scratch/fig%206-18.png" alt="6-18">   </p>
</li>
<li><p>위의 $g$로 새로 추가되는 정보가 얼마나 가치있는 정보인지 판단하기 위해 input gate로 가중치를 줌</p>
<script type="math/tex; mode=display">i = \sigma(x_{t} W_{x}^{(i)} + h_{t-1}W_{h}^{(i)} + b^{(i)})</script></li>
<li><p>이후 $i$와 $g$의 원소별 곱을 기억 셀 $c$에 추가</p>
</li>
</ul>
<h3 id="LSTM의-기울기-흐름"><a href="#LSTM의-기울기-흐름" class="headerlink" title="LSTM의 기울기 흐름"></a>LSTM의 기울기 흐름</h3><ul>
<li>기억 셀 c의 역전파 흐름을 보면 $+$와 $\times$ 노드를 지남  </li>
<li>$+$ 노드를 지날때는 기울기 변화가 일어나지 않음</li>
<li>$\times$ 노드의 경우 matrix multiplication이 아닌 hadamard product(elementwise)를 계산하기 때문에 기울기 소실이 일어나기 어려움</li>
<li>특히 $\times$ 연산을 제어하는 forget gate가 잊어야할 정보와 잊어서는 안될 정보를 판단해주기 때문에 ‘오래 기억해야 할 정보’의 경우 기울기가 소실 없이 전파될 것을 기대할 수 있음</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%206-19.png" alt="6-19"></p>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li>아래 네가지 게이트에서 일어나는 계산들은 $W$만 다를 뿐 같은 계산을 수행함.(Affine transformation)</li>
<li>4개의 가중치 $W$를 하나로 모아서 한번의 affine transformation만 수행할것임.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b</span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, h_prev, c_prev</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, H = h_prev.shape</span><br><span class="line">        </span><br><span class="line">        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># slice</span></span><br><span class="line">        f = A[:, :H]</span><br><span class="line">        g = A[:, H:<span class="number">2</span>*H]</span><br><span class="line">        i = A[:, <span class="number">2</span>*H:<span class="number">3</span>*H]</span><br><span class="line">        o = A[:, <span class="number">3</span>*H:]</span><br><span class="line">        </span><br><span class="line">        f = sigmoid(f)</span><br><span class="line">        g = np.tanh(g)</span><br><span class="line">        i = sigmoid(i)</span><br><span class="line">        o = sigmoid(o)</span><br><span class="line">        </span><br><span class="line">        c_next = f * c_prev + g * i</span><br><span class="line">        h_next = o * np.tanh(c_next)</span><br><span class="line">        </span><br><span class="line">        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)</span><br><span class="line">        <span class="keyword">return</span> h_next, c_next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dh_next, dc_next</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        x, h_prev, c_prev, i, f, g, o, c_next = self.cache</span><br><span class="line">        </span><br><span class="line">        tanh_c_next = np.tanh(c_next)</span><br><span class="line">        <span class="comment"># ?</span></span><br><span class="line">        ds = dc_next + (dh_next * o) * (<span class="number">1</span> - tanh_c_next **<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        dc_prev = ds * f</span><br><span class="line">        df = ds * c_prev</span><br><span class="line">        dg = ds * i</span><br><span class="line">        di = ds * g</span><br><span class="line">        do = dh_next * tanh_c_next</span><br><span class="line">        </span><br><span class="line">        df *= f * (<span class="number">1</span> - f)</span><br><span class="line">        dg *= (<span class="number">1</span> - g**<span class="number">2</span>)</span><br><span class="line">        di *= i * (<span class="number">1</span> - i)</span><br><span class="line">        do *= o * (<span class="number">1</span> - o)</span><br><span class="line">        </span><br><span class="line">        dA = np.hstack((df, dg, di, do))</span><br><span class="line">        </span><br><span class="line">        dWh = np.dot(h_prev.T, dA)</span><br><span class="line">        dWx = np.dot(x.T, dA)</span><br><span class="line">        db = dA.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dWx</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = dWh</span><br><span class="line">        self.grads[<span class="number">2</span>][...] = db</span><br><span class="line">        </span><br><span class="line">        dx = np.dot(dA, Wx.T)</span><br><span class="line">        dh_prev = np.dot(dA, Wh.T)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dx, dh_prev, dc_prev</span><br></pre></td></tr></table></figure>
<h3 id="Time-LSTM"><a href="#Time-LSTM" class="headerlink" title="Time LSTM"></a>Time LSTM</h3><ul>
<li>T개의 시계열 데이터를 한꺼번에 처리하는 T개의 LSTM layer</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeLSTM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b, stateful=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line">        self.h, self.c = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        self.dh = <span class="literal">None</span></span><br><span class="line">        self.stateful = stateful</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, D = xs.shape</span><br><span class="line">        H = Wh.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        self.layers = []</span><br><span class="line">        hs = np.empty((N, T, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.h <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.h = np.zeros((N, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.c <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.c = np.zeros((N, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = LSTM(*self.params)</span><br><span class="line">            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)</span><br><span class="line">            hs[:, t, :] = self.h</span><br><span class="line">            </span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> hs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dhs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, H = dhs.shape</span><br><span class="line">        D = Wx.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        dxs = np.empty((N, T, D), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        dh, dc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        grads = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(T)):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)</span><br><span class="line">            dxs[:, t, :] = dx</span><br><span class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(layer.grads):</span><br><span class="line">                grads[i] += grad</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> i, grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(grads):</span><br><span class="line">            self.grads[i][...] = grad</span><br><span class="line">        </span><br><span class="line">        self.dh = dh</span><br><span class="line">        <span class="keyword">return</span> dxs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span>(<span class="params">self, h, c=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.h, self.c = h, c</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.h, self.c = <span class="literal">None</span>, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="LSTM을-사용한-RNNLM"><a href="#LSTM을-사용한-RNNLM" class="headerlink" title="LSTM을 사용한 RNNLM"></a>LSTM을 사용한 RNNLM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rnnlm</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size=<span class="number">10000</span>, wordvec_size=<span class="number">100</span>, hidden_size=<span class="number">100</span></span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 가중치 초기화</span></span><br><span class="line">        embed_W = (rn(V, D) / <span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span> * H) / np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(H, V) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 계층 생성</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeAffine(affine_W, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.lstm_layer = self.layers[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 모든 가중치와 기울기를 리스트에 모은다.</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        <span class="keyword">return</span> xs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        score = self.predict(xs)</span><br><span class="line">        loss = self.loss_layer.forward(score, ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.lstm_layer.reset_state()</span><br></pre></td></tr></table></figure>
<h3 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> common.optimizer <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> common.trainer <span class="keyword">import</span> RnnlmTrainer</span><br><span class="line"><span class="keyword">from</span> common.util <span class="keyword">import</span> eval_perplexity</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">wordvec_size = <span class="number">100</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">time_size = <span class="number">35</span></span><br><span class="line">lr = <span class="number">20.0</span></span><br><span class="line">max_epoch = <span class="number">4</span></span><br><span class="line">max_grad = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load dataa</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">corpus_test, _, _ = ptb.load_data(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">xs = corpus[:-<span class="number">1</span>]</span><br><span class="line">ts = corpus[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model 생성</span></span><br><span class="line">model = Rnnlm(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = SGD(lr)</span><br><span class="line">trainer = RnnlmTrainer(model, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습</span></span><br><span class="line">trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval = <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>| current_epoch 1 |  iters 1 / 1327 | time 1.0497894287109375[s] | perplexity 9999.937519272651
| current_epoch 2 |  iters 1 / 1327 | time 976.033093214035[s] | perplexity 223.09635976041903
| current_epoch 3 |  iters 1 / 1327 | time 1924.3960175514221[s] | perplexity 162.90021814658576
| current_epoch 4 |  iters 1 / 1327 | time 2846.238212585449[s] | perplexity 134.39349578293525
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(trainer.ppl_list[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/image/nlp-from-scratch/out%206-1.png" alt="out6-2"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.reset_state()</span><br><span class="line">ppl_test = eval_perplexity(model, corpus_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test perplexity&#x27;</span>, ppl_test)</span><br></pre></td></tr></table></figure>
<pre><code>test perplexity 135.6399667974982
</code></pre><h2 id="RNNLM-개선"><a href="#RNNLM-개선" class="headerlink" title="RNNLM 개선"></a>RNNLM 개선</h2><ul>
<li>위의 RNNLM을 크게 3가지 방법을 통해 개선할 예정.<ol>
<li>LSTM 계층의 다층화</li>
<li>dropout</li>
<li>가중치 공유</li>
</ol>
</li>
</ul>
<h3 id="LSTM-계층의-다층화"><a href="#LSTM-계층의-다층화" class="headerlink" title="LSTM 계층의 다층화"></a>LSTM 계층의 다층화</h3><ul>
<li>LSTM layer를 여러 겹 쌓아서 모델의 정확도가 향상될 것을 기대.<ul>
<li>PTB dataset의 언어 모델에서는 보통 2~4층에서 좋은 결과를 얻었음</li>
</ul>
</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li>Dropout을 통해 overfitting 방지<ul>
<li>이떄 시계열 방향으로 dropout을 넣으면 시간의 흐름에 따라 정보가 사라질 수 있음</li>
<li>대신 layer 깊이 방향으로 dropout을 적용</li>
</ul>
</li>
<li>Variational Dropout : 같은 layer에 속한 dropout들끼리 mask를 공유하여 dropout을 시간 방향으로 적용.<br><img src="/image/nlp-from-scratch/fig%206-34.png" alt="6-34"></li>
</ul>
<h3 id="가중치-공유-Weight-tying"><a href="#가중치-공유-Weight-tying" class="headerlink" title="가중치 공유(Weight tying)"></a>가중치 공유(Weight tying)</h3><ul>
<li>Embedding layer와 Affine layer가 가중치를 공유함으로써 학습해야하는 parameter 수가 크게 줄어들고(계산량 감소 및 overfitting도 방지), 정확도도 향상<br><img src="/image/nlp-from-scratch/fig%206-35.png" alt="6-35"></li>
</ul>
<h3 id="구현-1"><a href="#구현-1" class="headerlink" title="구현"></a>구현</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BetterRnnlm</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size=<span class="number">10000</span>, wordvec_size=<span class="number">650</span>, hidden_size=<span class="number">650</span>, dropout_ratio=<span class="number">0.5</span></span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx1 = (rn(D, <span class="number">4</span> * H) / np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh1 = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b1 = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx2 = (rn(D, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh2 = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b2 = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeAffine(embed_W.T, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.lstm_layers = [self.layers[<span class="number">2</span>], self.layers[<span class="number">4</span>]]</span><br><span class="line">        self.drop_layers = [self.layers[<span class="number">1</span>], self.layers[<span class="number">3</span>], self.layers[<span class="number">5</span>]]</span><br><span class="line">        </span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, xs, train_flg=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.drop_layers:</span><br><span class="line">            layer.train_fig = train_fig</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        <span class="keyword">return</span> xs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts,  ustrain_flg=<span class="literal">True</span></span>):</span></span><br><span class="line">        score = self.predict(xs, train_flg)</span><br><span class="line">        loss = self.loss_layer.forward(score, ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.lstm_layers:</span><br><span class="line">            layer.reset_state()        </span><br></pre></td></tr></table></figure>
<h3 id="학습-1"><a href="#학습-1" class="headerlink" title="학습"></a>학습</h3><p>위 Rnnlm과 같은 조건으로 학습(epoch는 4번에서 40번으로 늘려서)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)</span><br><span class="line">optimizer = SGD(lr)</span><br><span class="line">trainer = RnnlmTrainer(model, optimizer)</span><br><span class="line"></span><br><span class="line">best_ppl = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">  trainer.fit(xs, ts, max_epoch=<span class="number">1</span>, batch_size=batch_size, time_size=time_size, max_grad=max_grad)</span><br><span class="line">  model.reset_state()</span><br><span class="line">  ppl = eval_perplexity(model, corpus_val)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;perplexity : <span class="subst">&#123;ppl&#125;</span>&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> best_ppl &gt; ppl:</span><br><span class="line">    best_ppl = ppl</span><br><span class="line">    model.save_params()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    lr /= <span class="number">4.0</span></span><br><span class="line">    optimizer.lr = lr</span><br><span class="line">  </span><br><span class="line">  model.reset_state()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h4 id="출력"><a href="#출력" class="headerlink" title="출력"></a>출력</h4><pre><code>| current_epoch 1 |  iters 1 / 1327 | time 0.4454360008239746[s] | perplexity 10000.160734654946
test perplexity : 199.91582600396907
| current_epoch 2 |  iters 1 / 1327 | time 0.42688775062561035[s] | perplexity 285.35975929723134
test perplexity : 146.85835254402008
| current_epoch 3 |  iters 1 / 1327 | time 0.401918888092041[s] | perplexity 220.99791134777234
test perplexity : 125.61738284895596
| current_epoch 4 |  iters 1 / 1327 | time 0.4348595142364502[s] | perplexity 193.85762995444483
test perplexity : 112.8790201600887
| current_epoch 5 |  iters 1 / 1327 | time 0.41679954528808594[s] | perplexity 171.20541935259646
test perplexity : 105.05696537135755
| current_epoch 6 |  iters 1 / 1327 | time 0.42535948753356934[s] | perplexity 158.62854755201397
test perplexity : 100.56030860396797
| current_epoch 7 |  iters 1 / 1327 | time 0.4258577823638916[s] | perplexity 146.75250942965903
test perplexity : 96.68474006387768
| current_epoch 8 |  iters 1 / 1327 | time 0.42501020431518555[s] | perplexity 141.41520295810776
test perplexity : 94.68295145752191
...
</code></pre><h4 id="epoch-40번-진행-후-평가"><a href="#epoch-40번-진행-후-평가" class="headerlink" title="epoch 40번 진행 후 평가"></a>epoch 40번 진행 후 평가</h4><pre><code>test perplexity : 76.2854392816185
</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/23/nlp/from-scratch/ch05_RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/23/nlp/from-scratch/ch05_RNN/" class="post-title-link" itemprop="url">Ch 05. RNN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-23 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-23T00:00:00+09:00">2021-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-25 16:57:45" itemprop="dateModified" datetime="2021-10-25T16:57:45+09:00">2021-10-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><ul>
<li>기존 feed forward 방식에서 시계열 데이터의 성질(패턴)을 충분히 학습하기 위해 RNN(Recurrent Neural Network) 도입.</li>
</ul>
<h2 id="확률과-언어-모델"><a href="#확률과-언어-모델" class="headerlink" title="확률과 언어 모델"></a>확률과 언어 모델</h2><ul>
<li>CBOW 모델의 목적 : 맥락(context)으로부터 타깃(target)을 정확하게 추측하는 것</li>
<li><p>언어 모델(Language Model) : 단어 나열에 확률을 부여함.</p>
<ul>
<li>단어의 sequence가 일어날 가능성이 어느정도인지 확률로 평가.(얼마나 자연스러운 단어 순서인지)</li>
<li>t-1번째 까지의 단어가 나왔을 때, t번째 단어가 나올 확률을 계산하여(조건부 언어 모델, Conditional Language Model) 동시 확률을 구함</li>
<li>Markov Chain을 통해 확률 근사<br><img src="/image/nlp-from-scratch/e%205-8.png" alt="e5-8"></li>
</ul>
</li>
<li><p>CBOW(Continuous Bag-Of-Words)는 ‘가방안에 들어 있는 단어들’이라는 이름에서 알 수 있듯이 가방 속의 단어의 순서는 무시함.</p>
</li>
<li>위 언어모델 확률 계산에서, 맥락의 크기를 고정하여(위의 경우 2) 확률을 근사했는데, CBOW의 경우 맥락의 크기 밖의 정보를 무시하는 문제가 발생함.<br/><img src="/image/nlp-from-scratch/fig%205-5.png" alt="fig5-5"></li>
<li>CBOW는 왼쪽 그림과 같이 단어 벡터들이 더해지면서 단어의 순서가 무시됨</li>
<li>신경 확률론적 언어모델(Neural Probablistic Language Model)에서는 오른쪽 그림과 같이 단어를 더하는 대신 연결(concatenate)하는 방식을 사용</li>
<li>이 때 W의 parameter가 늘어나는 문제를 RNN을 사용하여 해결!</li>
</ul>
<h2 id="RNN-1"><a href="#RNN-1" class="headerlink" title="RNN"></a>RNN</h2><ul>
<li><p>앞서 만들었던 단어의 분산 표현($x_t$)을 순서대로 RNN layer에 입력<br/></p>
<p> <img src="/image/nlp-from-scratch/fig%205-8.png" alt="fig5-8"></p>
</li>
<li><p>각 hidden layer에서 두 input을 바탕으로 output 계산<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-9.png" alt="fig5-9"></p>
</li>
<li><p>이때 $t$번째, ‘시각 $t$’에서의 출력 $h_t$를 은닉 상태($hidden~state$)라고 함</p>
</li>
</ul>
<h3 id="BPTT"><a href="#BPTT" class="headerlink" title="BPTT"></a>BPTT</h3><ul>
<li><p>BPTT(Backpropagation Through Time) : 시간 방향으로 펼친 신경망의 오차역전파법<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-10.png" alt="fig5-10"></p>
</li>
<li><p>그러나 BPTT을 이용할 때, 계산량 뿐만 아니라 RNN layer 중간 데이터를 메모리에 유지해 두어야 하기 때문에 메모리 사용량도 증가</p>
</li>
</ul>
<h3 id="Truncated-BPTT"><a href="#Truncated-BPTT" class="headerlink" title="Truncated BPTT"></a>Truncated BPTT</h3><ul>
<li><p>길어진 신경망 연결을 적당한 길이로 끊어서 계산량을 줄임</p>
</li>
<li><p>layer가 많아지면서 기울기 값이 점점 작아져서 0에 가까워지는 문제도 해결</p>
</li>
<li><p>forward propagation은 유지하고 backward의 연결만 적당한 길이로 끊어야 함<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-11.png" alt="fig5-11"></p>
</li>
<li><p>RNN 학습시에 forward propagation를 순서대로 수행하고 그 다음 backward propagaion을 수행함</p>
</li>
<li><p>그러므로 기존 신경망에서 미니배치 학습을 수행할 떄 데이터를 무작위로 선택했던 것과 다르게 데이터를 순서대로 입력해야 함.</p>
</li>
</ul>
<h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><ul>
<li>랜덤하게 미니 배치를 선택했던 것과 다르게 데이터를 순서대로 입력해야함.</li>
<li>각 미니배치마다 시작 위치를 batch-size만큼 옮겨줘야 함</li>
</ul>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li><p>길이가 T인 시계열 데이터를 받아서, 각 시각의 hidden state T개를 출력.</p>
</li>
<li><p>이때 입력($xs$), 출력($hs$)를 묶은 layer를 Time RNN layer라고 함<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-17.png" alt="fig5-17"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RNN layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b</span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, h_prev</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b</span><br><span class="line">        h_next = np.tanh(t)</span><br><span class="line">        </span><br><span class="line">        self.cache = (x, h_prev, h_next)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> h_next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dh_next</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        x, h_prev, h_next = self.cache</span><br><span class="line">        </span><br><span class="line">        dt = dh_next * (<span class="number">1</span> - h_next ** <span class="number">2</span>)</span><br><span class="line">        db = np.<span class="built_in">sum</span>(dt, axis=<span class="number">0</span>)</span><br><span class="line">        dWh = np.dot(h_prev.T, dt)</span><br><span class="line">        dh_prev = np.dot(dt, Wh.T)</span><br><span class="line">        dWx = np.dot(x.T, dt)</span><br><span class="line">        dx = np.dot(dt, Wx.T)</span><br><span class="line">        </span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dWx</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = dWh</span><br><span class="line">        self.grads[<span class="number">2</span>][...] = db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dx, dh_prev</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="Time-RNN"><a href="#Time-RNN" class="headerlink" title="Time RNN"></a>Time RNN</h3><ul>
<li>stateful(True/False) : Time RNN이 hidden state를 유지할지</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeRNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b, stateful=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.h, self.dh = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        self.stateful = stateful</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, D = xs.shape</span><br><span class="line">        D, H = Wx.shape</span><br><span class="line">        </span><br><span class="line">        self.layers = []</span><br><span class="line">        hs = np.empty((N, T, H), dtype=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.h <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.h = np.zeros((N, H), dtype=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = RNN(*self.params)</span><br><span class="line">            self.h = layer.forward(xs[:, t, :], self.h)</span><br><span class="line">            hs[:, t, :] = self.h</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> hs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dhs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, H = dhs.shape</span><br><span class="line">        D, H = Wx.shape</span><br><span class="line">        </span><br><span class="line">        dxs = np.empty((N, T, D), dtype=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        grads = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(T)):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dx, dh = layer.backward(dhs[:, t, :] + dh)</span><br><span class="line">            dxs[:, t, :] = dx</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(grads):</span><br><span class="line">                self.grads[i][...] = grad</span><br><span class="line">            self.dh = dh</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dxs</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span>(<span class="params">self, h</span>):</span></span><br><span class="line">        self.h = h</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.h = <span class="literal">None</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h3 id="RNNLM-RNN-Language-Model"><a href="#RNNLM-RNN-Language-Model" class="headerlink" title="RNNLM(RNN Language Model)"></a>RNNLM(RNN Language Model)</h3><ol>
<li><p>문장을 단어의 단어들을 단어의 분산 표현(vector)로 변환</p>
</li>
<li><p>해당 vector들을 순서대로 RNN layer에 입력</p>
</li>
<li><p>RNNLM은 지금까지 입력된 단어를 기억하고 다음에 출현할 단어를 예측<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-25.png" alt="fig5-25"></p>
</li>
</ol>
<h3 id="Time-layers"><a href="#Time-layers" class="headerlink" title="Time layers"></a>Time layers</h3><ul>
<li><p>각 layer 종류 XX마다 Time XX layer를 구현</p>
<ul>
<li>Time Affine</li>
<li>Time Embedding</li>
<li>Time Softmax with Loss : T개의 Softmax with Loss layer 각각의 loss를 평균</li>
<li>Time Sigmoid with Loss</li>
</ul>
</li>
<li><p>Time Affine</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAffine</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W, b</span>):</span></span><br><span class="line">        self.params = [W, b]</span><br><span class="line">        self.grads = [np.zeros_like(W), np.zeros_like(b)]</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        N, T, D = x.shape</span><br><span class="line">        W, b = self.params</span><br><span class="line">        </span><br><span class="line">        rx = x.reshape(N*T, -<span class="number">1</span>)</span><br><span class="line">        out = np.dot(rx, W) + b</span><br><span class="line">        self.x = x</span><br><span class="line">        <span class="keyword">return</span> out.reshape(N, T, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        x = self.x</span><br><span class="line">        N, T, D = x.shape</span><br><span class="line">        W, b = self.params</span><br><span class="line">        </span><br><span class="line">        dout = dout.reshape(N*T, -<span class="number">1</span>)</span><br><span class="line">        rx = x.reshape(N*T, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        dx = np.dot(dout, W.T)</span><br><span class="line">        dx = dx.reshape(*x.shape)</span><br><span class="line">        dW = np.dot(rx.T, dout)</span><br><span class="line">        db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dW</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = db</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<ul>
<li>Time Embedding</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeEmbedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W</span>):</span></span><br><span class="line">        self.params = [W]</span><br><span class="line">        self.grads = [np.zeros_like(W)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line">        self.W = W</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        N, T = xs.shape</span><br><span class="line">        V, D = self.W.shape</span><br><span class="line"></span><br><span class="line">        output = np.empty((N, T, D), dtype=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        self.layers = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = Embedding(self.W)</span><br><span class="line">            output[:, t, :] = layer.forward(xs[:, t])</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        N, T, D = dout.shape</span><br><span class="line">        </span><br><span class="line">        grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            layer.backward(dout[:, t, :])</span><br><span class="line">            grad += layer.grads[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">        self.grads[<span class="number">0</span>][...] = grad</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Time Softmax with Loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeSoftmaxWithLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        self.ignore_label = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        N, T, V = xs.shape</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> ts.ndim == <span class="number">3</span>: <span class="comment"># 정답 label이 one-hot vector일 때</span></span><br><span class="line">            ts = ts.argmax(axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ignore_label에 해당하는 데이터 처리</span></span><br><span class="line">        mask = (ts != self.ignore_label)</span><br><span class="line">        </span><br><span class="line">        xs = xs.reshape(N * T, V)</span><br><span class="line">        ts = ts.reshape(N * T)</span><br><span class="line">        mask = mask.reshape(N * T)</span><br><span class="line">        </span><br><span class="line">        ys = softmax(xs)</span><br><span class="line">        ls = mask * np.log(ys[np.arange(N * T), ts]) <span class="comment"># ignore_label에 해당하는 데이터 손실 0으로 처리</span></span><br><span class="line">        loss = -np.<span class="built_in">sum</span>(ls) / mask.<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        self.cache = (ts, ys, mask, (N, T, V))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        ts, ys, mask, (N, T, V) = self.cache</span><br><span class="line">        </span><br><span class="line">        dx = ys</span><br><span class="line">        dx[np.arange(N * T), ts] -= <span class="number">1</span></span><br><span class="line">        dx = dx * dout / mask.<span class="built_in">sum</span>()</span><br><span class="line">        dx *= mask[:, np.newaxis] <span class="comment"># ignoer_label에 해당하는 데이터 기울기를 0으로</span></span><br><span class="line">        </span><br><span class="line">        dx = dx.reshape((N, T, V))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<ul>
<li>Time Sigmoid with Loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeSigmoidWithLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.xs_shape = <span class="literal">None</span></span><br><span class="line">        self.layers= <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        N, T = xs.shape</span><br><span class="line">        self.xs_shape = xs.shape</span><br><span class="line">        </span><br><span class="line">        self.layers = []</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = SigmoidWithLoss()</span><br><span class="line">            loss += layer.forward(xs[:, t], ts[:, t])</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> loss/T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        N, T = self.xs_shape</span><br><span class="line">        dxs = np.empty(self.xs_shape, dtype=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        dout /= T</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dxs[:, t] = layer.backward(dout)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dxs</span><br></pre></td></tr></table></figure>
<h3 id="RNNLM-구현"><a href="#RNNLM-구현" class="headerlink" title="RNNLM 구현"></a>RNNLM 구현</h3><ul>
<li><p>SimpleRnnlm : 아래와 같이 4개의 time layer를 쌓은 신경망<br/></p>
<p><img src="/image/nlp-from-scratch/fig%205-30.png" alt="fig5-30"></p>
</li>
<li><p>Xavier initialization : 이전 layer의 노드의 개수($n$)을 사용해 표준편차가 $1 \over \sqrt n$인 분포로 initialize</p>
<ul>
<li>non-linear(tanh, sigmoid 등)에서 좋은 성능</li>
<li>ReLU의 경우 출력값이 0으로 수렴하는 문제가 있음</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleRnnlm</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize weight(Xavier)</span></span><br><span class="line">        embed_W = (rn(V, D) / <span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        rnn_b = np.zeros(H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(H, V) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer 생성</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeAffine(affine_W, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.rnn_layer = self.layers[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># list</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        loss = self.loss_layer.forward(xs, ts)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        hidden state 초기화</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.rnn_layer.reset_state()</span><br></pre></td></tr></table></figure>
<h2 id="평가"><a href="#평가" class="headerlink" title="평가"></a>평가</h2><h3 id="perplexity"><a href="#perplexity" class="headerlink" title="perplexity"></a>perplexity</h3><ul>
<li>‘확률의 역수’ : 값이 낮을수록 언어 모델의 예측 성능이 좋은 모델</li>
<li>$perplexity = e^L =  exp~(- {1 \over N} \sum\limits<em>{n} \sum\limits</em>{k} t<em>{nk}~log y</em>{nk})$</li>
<li>$N$ : 데이터의 총 개수, $t<em>n$ : 정답 레이블, $t</em>{nk}$ : n개째 데이터의 k번째 값, $y_{nk}$ : 확률분포</li>
<li>위 값들을 통해 계산한 $L$ 은 Cross-entroy error를 통해 계산한 신경망의 손실과 같음</li>
</ul>
<h2 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h2><h4 id="hyperparameter-설정"><a href="#hyperparameter-설정" class="headerlink" title="hyperparameter 설정"></a>hyperparameter 설정</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hyperparameter 설정</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">wordvec_size = <span class="number">100</span></span><br><span class="line">hidden_size = <span class="number">100</span> <span class="comment"># RNN의 hidden state의 벡터의 길이</span></span><br><span class="line">time_size = <span class="number">5</span> <span class="comment"># Truncated BPTT가 한번에 펼치는 시간의 길이</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">max_epoch = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<h4 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data load</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">corpus_size = <span class="number">1000</span></span><br><span class="line">corpus = corpus[:corpus_size]</span><br><span class="line">vocab_size = <span class="built_in">int</span>(<span class="built_in">max</span>(corpus)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">xs = corpus[:-<span class="number">1</span>]</span><br><span class="line">ts = corpus[<span class="number">1</span>:]</span><br><span class="line">data_size = <span class="built_in">len</span>(xs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;corpus size : <span class="subst">&#123;corpus_size&#125;</span>, vocab size : <span class="subst">&#123;vocab_size&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corpus size : 1000, vocab size : 418</span><br></pre></td></tr></table></figure>
<h4 id="initialize"><a href="#initialize" class="headerlink" title="initialize"></a>initialize</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 변수 설정</span></span><br><span class="line">max_iters = data_size // (batch_size * time_size)</span><br><span class="line">time_idx = <span class="number">0</span></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line">loss_count = <span class="number">0</span></span><br><span class="line">ppl_list = []</span><br></pre></td></tr></table></figure>
<h4 id="model-생성"><a href="#model-생성" class="headerlink" title="model 생성"></a>model 생성</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 생성</span></span><br><span class="line">model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = SGD(lr)</span><br></pre></td></tr></table></figure>
<h4 id="학습-1"><a href="#학습-1" class="headerlink" title="학습"></a>학습</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 각 mini-batch에서 샘플을 읽는 시작 위치 계산</span></span><br><span class="line">jump = (corpus_size - <span class="number">1</span>) // batch_size</span><br><span class="line">offsets = [i * jump <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 각 epoch, iter마다 순차적으로</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    <span class="keyword">for</span> iters <span class="keyword">in</span> <span class="built_in">range</span>(max_iters):</span><br><span class="line">        <span class="comment"># mini-batch 생성</span></span><br><span class="line">        batch_x = np.empty((batch_size, time_size), dtype=<span class="string">&#x27;i&#x27;</span>)</span><br><span class="line">        batch_t = np.empty((batch_size, time_size), dtype=<span class="string">&#x27;i&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(time_size):</span><br><span class="line">            <span class="keyword">for</span> i, offset <span class="keyword">in</span> <span class="built_in">enumerate</span>(offsets):</span><br><span class="line">                batch_x[i, t] = xs[(offset + time_idx) % data_size]</span><br><span class="line">                batch_t[i, t] = ts[(offset + time_idx) % data_size]</span><br><span class="line">            time_idx += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 기울기 갱신</span></span><br><span class="line">        loss = model.forward(batch_x, batch_t)</span><br><span class="line">        model.backward()</span><br><span class="line">        optimizer.update(model.params, model.grads)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        loss_count += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 각 epoch마다 평가(perplexity)</span></span><br><span class="line">    ppl = np.exp(total_loss / loss_count)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;| epoch : <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span> | perplexity : <span class="subst">&#123;ppl&#125;</span> |&#x27;</span>)</span><br><span class="line">    ppl_list.append(<span class="built_in">float</span>(ppl))</span><br><span class="line">    total_loss, loss_count = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.plot(ppl_list)</span><br></pre></td></tr></table></figure>
<p><img src="/image/nlp-from-scratch/5-simplernnlm.png" alt="png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/22/nlp/from-scratch/ch04_CBOW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/22/nlp/from-scratch/ch04_CBOW/" class="post-title-link" itemprop="url">Ch 04. Word2vec 개선</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-22 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-22T00:00:00+09:00">2021-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-25 16:26:18" itemprop="dateModified" datetime="2021-10-25T16:26:18+09:00">2021-10-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Simple-CBOW의-문제점"><a href="#Simple-CBOW의-문제점" class="headerlink" title="Simple CBOW의 문제점"></a>Simple CBOW의 문제점</h2><ul>
<li>window size를 1로 한정했음<ul>
<li>원하는 window size에 대해 계산할 수 있도록 개선 </li>
</ul>
</li>
<li>어휘수가 많아질수록 계산량이 매우 커짐</li>
<li>특히 두가지 과정이 bottleneck<ol>
<li>input(one-hot vector)와 weight(W_in)의 곱셈</li>
<li>hidden layer 에서 output으로 갈 때 weight(W_out)의 곱셈, softmax 계산</li>
</ol>
</li>
<li>해결 방안<ol>
<li>Embedding layer를 통해 계산량 감소</li>
<li>Negative sampling을 통해 loss 계산 과정 개선</li>
</ol>
</li>
</ul>
<h2 id="Embedding-layer"><a href="#Embedding-layer" class="headerlink" title="Embedding layer"></a>Embedding layer</h2><ul>
<li><p>one-hot vector와 W_in을 곱하는 과정은 W_in의 특정 row를 추출하는 것과 같음</p>
<p><img src="/image/nlp-from-scratch/fig%204-3.png" alt="4-3"></p>
</li>
<li><p>matrix 연산을 하지 않고 row vector를 추출하는 <code>Embedding layer</code>를 통해 계산량 개선</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W</span>):</span></span><br><span class="line">        self.params = [W]</span><br><span class="line">        self.grads = [np.zeros_like(W)]</span><br><span class="line">        self.idx = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        W, = self.params</span><br><span class="line">        output = W[self.idx]</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        dW, = self.grads</span><br><span class="line">        dW[...] = <span class="number">0</span> <span class="comment"># </span></span><br><span class="line"><span class="comment">#         # 좋지 않은 예. idx의 원소가 중복인 경우 문제가 생김</span></span><br><span class="line"><span class="comment">#         dW[self.idx] = dout </span></span><br><span class="line"><span class="comment">#         for i, word_id in enumerate(self.idx):</span></span><br><span class="line"><span class="comment">#             dW[word_id] += dout[i]</span></span><br><span class="line">        np.add.at(dW, self.idx, dout) <span class="comment"># cupy 버전 문제로 gpu 사용 시 수정해야 함</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Multi-class-gt-binary"><a href="#Multi-class-gt-binary" class="headerlink" title="Multi-class -&gt; binary"></a>Multi-class -&gt; binary</h2><ul>
<li>다중 분류(multi-class classification)을 이진 분류(binary classification)으로 근사하려는 것 <ul>
<li>해당 자리에 target이 나타날 확률 -&gt; 해당 자리에 위치한 단어가 target인지 아닌지로 변환  </li>
</ul>
</li>
<li><p>W_out의 target에 해당하는 column만 추출 후 hidden layer 뉴런과 내적 (Matrix x vector를 vector간 내적으로 변환)</p>
<ul>
<li>Multi-class 에서 Softmax -&gt; cross-entropy loss를 사용했던 것에서</li>
<li>Binary의 경우 Sigmoid -&gt; cross-entropy loss 사용 </li>
</ul>
</li>
<li><p>이때 Sigmoid layer와 Cross-entropy를 섞어서 back-propagation시 미분값이 $y-t$로 간단함.<br><img src="/image/nlp-from-scratch/fig%204-10.png" alt="4-10"></p>
</li>
</ul>
<h3 id="전체-과정"><a href="#전체-과정" class="headerlink" title="전체 과정"></a>전체 과정</h3><p><img src="/image/nlp-from-scratch/fig%204-12.png" alt="4-12"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># h와 W_out의 embedding vector의 dot 부분 구현</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingDot</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W</span>):</span></span><br><span class="line">        self.embed = Embedding(W)</span><br><span class="line">        self.params = self.embed.params</span><br><span class="line">        self.grads = self.embed.grads</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h, idx</span>):</span></span><br><span class="line">        target_W = self.embed.forward(idx) <span class="comment"># row 추출</span></span><br><span class="line">        out = np.<span class="built_in">sum</span>(target_W + h, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.cache = (h, target_W)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        h, target_W = self.cache</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        dout = dout.reshape(dout.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        dtarget_W = dout * h</span><br><span class="line">        self.embed.backward(dtarget_W)</span><br><span class="line">        dh = dout * target_W</span><br><span class="line">        <span class="keyword">return</span> dh</span><br></pre></td></tr></table></figure>
<h2 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h2><ul>
<li>binary classification으로 변환함으로써 계산량을 줄였음.</li>
<li>하지만 정답(positive)에 대해서만 학습했고, 오답(negative)에 대해서는 학습을 안함<ul>
<li>정답을 입력했을때 출력이 1에 가까운 방향으로 학습했음.</li>
<li>하지만 오답을 입력했을때 출력이 0에 가까워지도록 만들어줘야함.</li>
</ul>
</li>
<li>모든 negative에 대해 학습을 하기에는 어휘 수가 너무 늘어남<ul>
<li>적은 수의 negative를 sampling해서 사용.</li>
</ul>
</li>
</ul>
<h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><ul>
<li>자주 등장하는 단어가 많이 sampling되도록 각 단어의 출현 횟수의 확률분포에 따라 단어를 sampling.</li>
<li>negative를 적게 sampling하므로 자주 나오지 않는 단어에 대해 학습하는 것 보단 흔하게 나오는 단어를 사용하는 것이 나음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 직접 구현한 것인데 아래 list comprehension이 너무 오래 걸려서 수정 필요</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnigramSampler</span>:</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    corpus에서 target에 대한 negative sampling</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, corpus, power, sample_size</span>):</span></span><br><span class="line">        self.corpus = corpus</span><br><span class="line">        self.power = power</span><br><span class="line">        self.sample_size = sample_size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_negative_sample</span>(<span class="params">self, target</span>):</span></span><br><span class="line">        neg_sample = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> target:</span><br><span class="line">            <span class="comment"># words : target을 제외한 단어 집합</span></span><br><span class="line">            words = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">max</span>(corpus)+<span class="number">1</span>))</span><br><span class="line">            words.remove(t)</span><br><span class="line">            <span class="comment"># 확률분포 계산 </span></span><br><span class="line">            p = [<span class="built_in">list</span>(corpus).count(word)/<span class="built_in">len</span>(corpus) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            new_p = np.power(p, power)</span><br><span class="line">            new_p /= np.<span class="built_in">sum</span>(new_p)</span><br><span class="line">            neg = np.random.choice(words, sample_size, p = new_p, replace=<span class="literal">False</span>)</span><br><span class="line">            neg_sample.append(neg)</span><br><span class="line">        <span class="keyword">return</span> neg_sample</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">corpus = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">power = <span class="number">0.75</span></span><br><span class="line">sample_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sampler = UnigramSampler(corpus, power, sample_size)</span><br><span class="line">target = np.array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">0</span>])</span><br><span class="line">negative_sample = sampler.get_negative_sample(target)</span><br><span class="line"><span class="built_in">print</span>(negative_sample)</span><br></pre></td></tr></table></figure>
<pre><code>[array([3, 4]), array([0, 4]), array([4, 2])]
</code></pre><h3 id="Negative-Sampling-구현"><a href="#Negative-Sampling-구현" class="headerlink" title="Negative Sampling 구현"></a>Negative Sampling 구현</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NegativeSamplingLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W, corpus, power = <span class="number">0.75</span>, sample_size = <span class="number">5</span></span>):</span></span><br><span class="line">        self.sampler = UnigramSampler(corpus, power, sample_size)</span><br><span class="line">        self.sample_size = sample_size <span class="comment"># 샘플링 횟수</span></span><br><span class="line">        self.loss_layer = [SigmoidWithLoss() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (sample_size + <span class="number">1</span>)]</span><br><span class="line">        self.embed_dot_layers = [EmbeddingDot(W) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size + <span class="number">1</span>)]</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.embed_dot_layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h, target</span>):</span></span><br><span class="line">        batch_size = target.shape[<span class="number">0</span>]</span><br><span class="line">        neg_sample = self.sampler.get_negative_sample(target)</span><br><span class="line">        <span class="comment"># positive</span></span><br><span class="line">        score = self.embed_dot_layers[<span class="number">0</span>].forward(h, target)</span><br><span class="line">        pos_label = np.ones(batch_size).astype(<span class="built_in">int</span>)</span><br><span class="line">        loss = self.loss_layers[<span class="number">0</span>].forward(score, pos_label)</span><br><span class="line">        <span class="comment"># negative</span></span><br><span class="line">        neg_label = np.zeors(batcs_size).astype(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.sample_size):</span><br><span class="line">            neg_target = neg_sample[:, i]</span><br><span class="line">            score = self.embed_dot_layers[i].forward(h, neg_target)</span><br><span class="line">            loss = self.loss_layers[i].forward(score, neg_label)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> l0, l1 <span class="keyword">in</span> <span class="built_in">zip</span>(self.loss_layers, self.embed_dot_layers):</span><br><span class="line">            dscore = l0.backwawrd(dout)</span><br><span class="line">            dh += l1.backward(dscore)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> dh</span><br></pre></td></tr></table></figure>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><ul>
<li>Embedding, Negative Sampling을 적용하여 개선된 CBOW 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_size, window_size, corpus</span>):</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize weight</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        W_out = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer 생성</span></span><br><span class="line">        self.in_layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> * window_size):</span><br><span class="line">            layer = Embedding(W_in)</span><br><span class="line">            self.in_layers.append(layer)</span><br><span class="line">        self.neg_loss = NegativeSamplingLoss(W_out, corpus, power=<span class="number">0.75</span>, sample_size=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer, parameter 정리</span></span><br><span class="line">        layers = self.in_layers + [self.neg_loss]</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 단어의 분산 표현 저장</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, contexts, target</span>):</span></span><br><span class="line">        h = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.in_layers):</span><br><span class="line">            h += layer.forward(contexts[:, i])</span><br><span class="line">        h /= <span class="built_in">len</span>(self.in_layers)</span><br><span class="line">        loss = self.neg_loss.forward(h, target)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.neg_loss.backward(dout)</span><br><span class="line">        dout /= <span class="built_in">len</span>(self.in_layers)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.in_layers:</span><br><span class="line">            layer.backward(dout)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><ul>
<li>마찬가지로 개선된 Skip-gram 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGram</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_size, window_size, corpus</span>):</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize weight</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        W_out = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer 생성</span></span><br><span class="line">        self.in_layer = Embedding(W_in)</span><br><span class="line">        self.loss_layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> * window_size):</span><br><span class="line">            layer = NegativeSamplingLoss(W_out, corpus, power=<span class="number">0.75</span>, sample_size=<span class="number">5</span>)</span><br><span class="line">            self.loss_layers.append(layer)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer, parameter 정리</span></span><br><span class="line">        layers = [self.in_layer] + self.loss_layers</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 단어의 분산 표현 저장</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line">          </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, contexts, target</span>):</span></span><br><span class="line">        h = self.in_layer.forward(target)</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.loss_layers):</span><br><span class="line">            loss = layer.forward(h, contexts[:, i])</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.loss_layers):</span><br><span class="line">            dh += layer.backward(dout)</span><br><span class="line">        self.in_layer.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h2><h3 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hyperparameter</span></span><br><span class="line">window_size = <span class="number">5</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">max_epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">contexts, target = create_contexts_target(corpus, window_size)</span><br></pre></td></tr></table></figure>
<h3 id="CBOW-1"><a href="#CBOW-1" class="headerlink" title="CBOW"></a>CBOW</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = CBOW(vocab_size, hidden_size, window_size, corpus)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line">trainer.fit(contexts, target, max_epoch, batch_size)</span><br></pre></td></tr></table></figure>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="/image/nlp-from-scratch/4-cbow.png" alt="cbow"></p>
<h4 id="Most-similar-words"><a href="#Most-similar-words" class="headerlink" title="Most similar words"></a>Most similar words</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">you</th>
<th style="text-align:center">year</th>
<th style="text-align:center">car</th>
<th style="text-align:center">toyota</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">we (0.7427)</td>
<td style="text-align:center">month (0.8423)</td>
<td style="text-align:center">window (0.6211)</td>
<td style="text-align:center">seita (0.6777)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">i (0.7134)</td>
<td style="text-align:center">summer (0.7666)</td>
<td style="text-align:center">luxury (0.5762)</td>
<td style="text-align:center">honda (0.6548)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">your (0.6172)</td>
<td style="text-align:center">week (0.7637)</td>
<td style="text-align:center">cars (0.5723)</td>
<td style="text-align:center">chevrolet (0.6465)</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">someone (0.5820)</td>
<td style="text-align:center">spring (0.7329)</td>
<td style="text-align:center">auto (0.5703)</td>
<td style="text-align:center">mazda (0.6201)</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">weird (0.5801)</td>
<td style="text-align:center">decade (0.6812)</td>
<td style="text-align:center">merkur (0.5547)</td>
<td style="text-align:center">nissan (0.6118)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Analogy"><a href="#Analogy" class="headerlink" title="Analogy"></a>Analogy</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">king:man = queen:?</th>
<th style="text-align:center">take:took = go:?</th>
<th style="text-align:center">car:cars = child:?</th>
<th style="text-align:center">good:better = bad:?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">woman (5.3945)</td>
<td style="text-align:center">‘re (4.7070)</td>
<td style="text-align:center">a.m (6.7891)</td>
<td style="text-align:center">rather (5.5977)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">lady (4.5469)</td>
<td style="text-align:center">were (4.1641)</td>
<td style="text-align:center">rape (5.6953)</td>
<td style="text-align:center">more (5.4531)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">kid (4.3555)</td>
<td style="text-align:center">eurodollars (4.1094)</td>
<td style="text-align:center">daffynition (5.3945)</td>
<td style="text-align:center">less (5.2461)</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">recital (4.3125)</td>
<td style="text-align:center">are (3.9766)</td>
<td style="text-align:center">children: (5.2969)</td>
<td style="text-align:center">greater (4.2148)</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">father (4.2617)</td>
<td style="text-align:center">came (3.9668)</td>
<td style="text-align:center">women (4.9609)</td>
<td style="text-align:center">faster (3.8555)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Skip-gram-1"><a href="#Skip-gram-1" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = SkipGram(vocab_size, hidden_size, window_size, corpus)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line">trainer.fit(contexts, target, max_epoch, batch_size)</span><br></pre></td></tr></table></figure>
<h4 id="Loss-1"><a href="#Loss-1" class="headerlink" title="Loss"></a>Loss</h4><p><img src="/image/nlp-from-scratch/4-skipgram.png" alt="cbow"></p>
<h4 id="Most-similar-words-1"><a href="#Most-similar-words-1" class="headerlink" title="Most similar words"></a>Most similar words</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">you</th>
<th style="text-align:center">year</th>
<th style="text-align:center">car</th>
<th style="text-align:center">toyota</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">yourself (0.6670)</td>
<td style="text-align:center">month (0.6309)</td>
<td style="text-align:center">cars (0.5928)</td>
<td style="text-align:center">motor (0.7236)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">your (0.6660)</td>
<td style="text-align:center">week (0.6084)</td>
<td style="text-align:center">auto (0.5693)</td>
<td style="text-align:center">honda (0.6719)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">weird (0.6567)</td>
<td style="text-align:center">earlier (0.5762)</td>
<td style="text-align:center">luxury (0.5635)</td>
<td style="text-align:center">lexus (0.6675)</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">i (0.6494)</td>
<td style="text-align:center">quarter (0.5537)</td>
<td style="text-align:center">chevrolet (0.5552)</td>
<td style="text-align:center">infiniti (0.6396)</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">ask (0.6152)</td>
<td style="text-align:center">decade (0.5269)</td>
<td style="text-align:center">truck (0.5454)</td>
<td style="text-align:center">minivans (0.5870)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Analogy-1"><a href="#Analogy-1" class="headerlink" title="Analogy"></a>Analogy</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">king:man =  queen:?</th>
<th style="text-align:center">take:took =  go:?</th>
<th style="text-align:center">car:cars =  child:?</th>
<th style="text-align:center">good:better  = bad:?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">teacher (1.8574)</td>
<td style="text-align:center">yards (1.8213)</td>
<td style="text-align:center">rape (2.3008)</td>
<td style="text-align:center">worse (1.7891)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">discarded (1.7959)</td>
<td style="text-align:center">ran (1.6133)</td>
<td style="text-align:center">children (1.9131)</td>
<td style="text-align:center">gyrations (1.6816)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">hero (1.7451)</td>
<td style="text-align:center">amsterdam (1.4795)</td>
<td style="text-align:center">patients (1.8662)</td>
<td style="text-align:center">rough (1.6816)</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">kasparov (1.7334)</td>
<td style="text-align:center">crowded (1.4561)</td>
<td style="text-align:center">versions (1.8359)</td>
<td style="text-align:center">ever (1.5303)</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">murdered (1.7236)</td>
<td style="text-align:center">marched (1.4512)</td>
<td style="text-align:center">non-violent (1.7989)</td>
<td style="text-align:center">clobbered (1.5166)</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/18/nlp/from-scratch/ch03_word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/18/nlp/from-scratch/ch03_word2vec/" class="post-title-link" itemprop="url">Ch 03. Word2vec</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-18T00:00:00+09:00">2021-10-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-25 15:21:14" itemprop="dateModified" datetime="2021-10-25T15:21:14+09:00">2021-10-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="추론-기반-기법"><a href="#추론-기반-기법" class="headerlink" title="추론 기반 기법"></a>추론 기반 기법</h2><h3 id="기존-통계-기반-기법의-문제점"><a href="#기존-통계-기반-기법의-문제점" class="headerlink" title="기존 통계 기반 기법의 문제점"></a>기존 통계 기반 기법의 문제점</h3><ul>
<li>단어의 개수가 $N$개일 때 Co-occurence matrix의 크기는 $N \times N$으로 너무 큼.</li>
<li>SVD를 사용하여 차원 축소를 진행하는데, 이때 SVD를 적용하는 비용도 $O(n^3)$으로 너무 큼.</li>
</ul>
<h3 id="개선-방법"><a href="#개선-방법" class="headerlink" title="개선 방법"></a>개선 방법</h3><ul>
<li>Neural Network처럼 학습 데이터를 미니배치로 나눠서 순차적으로 학습.</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%203-1.png" alt="3-1"></p>
<h2 id="CBOW-Continuous-Bag-Of-Words"><a href="#CBOW-Continuous-Bag-Of-Words" class="headerlink" title="CBOW(Continuous Bag-Of-Words)"></a>CBOW(Continuous Bag-Of-Words)</h2><ul>
<li>주변 단어의 맥락(context)이 주어졌을때 무슨 단어(target)가 들어갈지 추측하는 과정.</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%203-2.png" alt="3-2"></p>
<ul>
<li>학습시 사용한 말뭉치(corpus)에 따라 얻게 되는 단어의 분산 표현이 다름.</li>
<li>가중치를 다시 학습할 수 있어서, 단어의 분산표현 갱신이나 새로운 단어 추가를 효육적으로 수행할 수 있음</li>
</ul>
<h3 id="학습-과정"><a href="#학습-과정" class="headerlink" title="학습 과정"></a>학습 과정</h3><p><img src="/image/nlp-from-scratch/fig%203-12.png" alt="3-12"></p>
<ol>
<li>context, target을 one-hot vector로 변환</li>
<li>해당 vector들을 input 으로 넣음 (이 때 input layer의 개수는 입력시키는 단어의 개수와 같음)</li>
<li>hidden layer에서 각 input layer들의 값의 평균을 계산</li>
<li>Softmax를 통해 해당 단어가 해당 자리에 나타날 확률을 계산</li>
<li>Cross-entropy error를 통해 Loss 계산, 학습</li>
</ol>
<h3 id="가중치"><a href="#가중치" class="headerlink" title="가중치"></a>가중치</h3><ul>
<li>CBOW 모델에는 가중치가 2개 존재함</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%203-15.png" alt="3-15"></p>
<ul>
<li>$W_{in}$의 각 행(row)이 각 단어의 분산표현을 나타냄</li>
<li>$W_{out}$에도 각 단어의 의미가 열(column)으로 저장됨</li>
<li>이 때 word2vec에서는 보통 $W_{in}$만 사용할 때 결과가 좋음(특히 skip-gram)</li>
<li>GloVE에서는 두 가중치를 더해서 사용했을 때 더 좋은 결과를 얻음</li>
</ul>
<h3 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCBOW</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_size</span>):</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize weight</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>) </span><br><span class="line">        W_out = <span class="number">0.01</span> * np.random.randn(H, V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer 생성</span></span><br><span class="line">        self.in_layer0 = MatMul(W_in)</span><br><span class="line">        self.in_layer1 = MatMul(W_in)</span><br><span class="line">        self.out_layer = MatMul(W_out)</span><br><span class="line">        self.loss_layer = SoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#list에 모으기</span></span><br><span class="line">        layers = [self.in_layer0, self.in_layer1, self.out_layer]</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 단어의 분산표현 저장</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, contexts, target</span>):</span></span><br><span class="line">        h0 = self.in_layer0.forward(contexts[:, <span class="number">0</span>])</span><br><span class="line">        h1 = self.in_layer1.forward(contexts[:, <span class="number">1</span>])</span><br><span class="line">        h = (h0 + h1) * <span class="number">0.5</span></span><br><span class="line">        score = self.out_layer.forward(h)</span><br><span class="line">        loss = self.loss_layer.forward(score, target)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        ds = self.loss_layer.backward(dout)</span><br><span class="line">        da = self.out_layer.backward(ds)</span><br><span class="line">        da *= <span class="number">0.5</span></span><br><span class="line">        self.in_layer1.backward(da)</span><br><span class="line">        self.in_layer0.backward(da)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><ul>
<li>CBOW와 다르게 target으로부터 주변 context를 추측하는 모델</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%203-23.png" alt="3-23"></p>
<ul>
<li>CBOW보다 계산 비용이 크지만 corpus가 커질수록 성능이 뛰어난 경향을 보임.</li>
</ul>
<h3 id="구현-1"><a href="#구현-1" class="headerlink" title="구현"></a>구현</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleSkipGram</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_size</span>):</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize weight</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">&#x27;f&#x27;</span>) </span><br><span class="line">        W_out = <span class="number">0.01</span> * np.random.randn(H, V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer 생성 : CBOW와 다르게 input layer가 1개, loss가 여러개</span></span><br><span class="line">        self.in_layer = MatMul(W_in)</span><br><span class="line">        self.out_layer = MatMul(W_out)</span><br><span class="line">        self.loss_layer1 = SoftmaxWithLoss()</span><br><span class="line">        self.loss_layer2 = SoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#list에 모으기</span></span><br><span class="line">        layers = [self.in_layer, self.out_layer, self.loss_layer1, self.loss_layer2]</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 단어의 분산표현 저장</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, contexts, target</span>):</span></span><br><span class="line">        h = self.in_layer.forward(target)</span><br><span class="line">        score = self.out_layer.forward(h)</span><br><span class="line">        loss = self.loss_layer1.forward(score, contexts[:, <span class="number">0</span>]) + self.loss_layer2.forward(score, contexts[:, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dscore = self.loss_layer1.backward(dout) + self.loss_layer2.backward(dout)</span><br><span class="line">        dh = self.out_layer.backward(dscore)</span><br><span class="line">        self.in_layer.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="통계-기반-vs-추론-기반"><a href="#통계-기반-vs-추론-기반" class="headerlink" title="통계 기반 vs 추론 기반"></a>통계 기반 vs 추론 기반</h2><ol>
<li>단어의 분산 표현을 수정하고 싶을 때<ul>
<li>통계 기반 기법은 co-occurence matrix를 새로 만들어야 하는 반면</li>
<li>추론 기반 기법(word2vec)은 기존 가중치를 초깃값으로 다시 학습하면 되기 때문에 효율적으로 갱신 가능함</li>
</ul>
</li>
<li>성능면에서는 통계 기반 / 추론 기반 기법의 우열을 따지기 어려움</li>
<li>두 기법은 연관이 있음<ul>
<li>skip-gram과 네거티브 샘플링을 이용한 모델은 co-occurence matrix에 특수한 행렬 분해를 적용한 것과 같음</li>
<li>GloVe : 두 기법을 융합한 기법</li>
</ul>
</li>
</ol>
<h2 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h2><h3 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">window_size = <span class="number">1</span></span><br><span class="line">hidden_size = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">max_epoch = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">text = <span class="string">&#x27;You say goodbye and I say hello.&#x27;</span></span><br><span class="line">corpus, word_to_id, id_to_word = preprocess(text)</span><br><span class="line"></span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">contexts, target = create_contexts_target(corpus, window_size)</span><br><span class="line">target = convert_one_hot(target, vocab_size)</span><br><span class="line">contexts = convert_one_hot(contexts, vocab_size)</span><br></pre></td></tr></table></figure>
<h3 id="Simple-CBOW"><a href="#Simple-CBOW" class="headerlink" title="Simple CBOW"></a>Simple CBOW</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = SimpleCBOW(vocab_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line">trainer.fit(contexts, target, max_epoch, batch_size, epoch_print = 100)</span><br><span class="line">trainer.plot()</span><br></pre></td></tr></table></figure>
<p><img src="/image/nlp-from-scratch/simple-cbow.png" alt="cbow"></p>
<h4 id="단어의-분산-표현"><a href="#단어의-분산-표현" class="headerlink" title="단어의 분산 표현"></a>단어의 분산 표현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_vecs = model.word_vecs</span><br><span class="line"><span class="keyword">for</span> word_id, word <span class="keyword">in</span> id_to_word.items():</span><br><span class="line">    <span class="built_in">print</span>(word, word_vecs[word_id])</span><br></pre></td></tr></table></figure>
<pre><code>you [-1.0762557  1.06351   -1.1074004  1.1025865 -1.5765114]
say [ 1.1755433 -1.1957825  1.1767687 -1.1574136  1.088242 ]
goodbye [-0.9526097   0.96037304 -0.86772525  0.9271136  -0.14380391]
and [ 0.6333946  -0.4580956   0.8180077  -0.80556715  1.8256714 ]
i [-0.9568056   0.941875   -0.8830476   0.92261046 -0.1562804 ]
hello [-1.062895   1.0663131 -1.1106303  1.0801047 -1.561029 ]
. [ 1.3913976 -1.4761357  1.2531542 -1.2582308 -0.9183479]
</code></pre><h3 id="Simple-skip-gram"><a href="#Simple-skip-gram" class="headerlink" title="Simple skip-gram"></a>Simple skip-gram</h3><h4 id="학습-1"><a href="#학습-1" class="headerlink" title="학습"></a>학습</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = SimpleSkipGram(vocab_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line">trainer.fit(contexts, target, max_epoch, batch_size, epoch_print = <span class="number">100</span>)</span><br><span class="line">trainer.plot()</span><br></pre></td></tr></table></figure>
<p><img src="/image/nlp-from-scratch/simple_skipgram.png" alt="skip-gram"></p>
<h4 id="단어의-분산-표현-1"><a href="#단어의-분산-표현-1" class="headerlink" title="단어의 분산 표현"></a>단어의 분산 표현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_vecs = model.word_vecs</span><br><span class="line"><span class="keyword">for</span> word_id, word <span class="keyword">in</span> id_to_word.items():</span><br><span class="line">    <span class="built_in">print</span>(word, word_vecs[word_id])</span><br></pre></td></tr></table></figure>
<pre><code>you [ 0.00152862  0.00943199  0.00498039 -0.01105605 -0.0090797 ]
say [ 0.40928003  0.9681609  -0.60750085  0.29873264  0.7500545 ]
goodbye [-0.6842289  -0.80604213  0.869326    1.2251657  -0.75968397]
and [ 1.1537582  0.9375379 -1.0481284 -1.3354286  1.0159786]
i [-0.66723204 -0.8001625   0.89004374  1.2151715  -0.761566  ]
hello [-1.1618923  -0.8838123   0.34882295 -0.72758615 -0.9388194 ]
. [-0.00162247  0.00803852 -0.00782559 -0.01251176  0.00806981]
</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/16/nlp/from-scratch/ch02_%EB%8B%A8%EC%96%B4%EC%9D%98%20%EB%B6%84%EC%82%B0%ED%91%9C%ED%98%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/16/nlp/from-scratch/ch02_%EB%8B%A8%EC%96%B4%EC%9D%98%20%EB%B6%84%EC%82%B0%ED%91%9C%ED%98%84/" class="post-title-link" itemprop="url">Ch 02. 단어의 분산 표현</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-16T00:00:00+09:00">2021-10-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-24 21:16:13" itemprop="dateModified" datetime="2021-10-24T21:16:13+09:00">2021-10-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="단어의-의미"><a href="#단어의-의미" class="headerlink" title="단어의 의미"></a>단어의 의미</h2><ul>
<li>자연어를 컴퓨터에게 이해시키기 위해서는 ‘단어의 의미’를 이해시키는 것이 중요함</li>
</ul>
<h3 id="단어의-의미를-표현하는-기법들"><a href="#단어의-의미를-표현하는-기법들" class="headerlink" title="단어의 의미를 표현하는 기법들"></a>단어의 의미를 표현하는 기법들</h3><ul>
<li>시소러스를 활용한 기법(WordNet)</li>
<li>통계 기반 기법(corpus)</li>
<li>추론 기반 기법(word2vec)</li>
</ul>
<h2 id="시소러스-thesaurus"><a href="#시소러스-thesaurus" class="headerlink" title="시소러스(thesaurus)"></a>시소러스(thesaurus)</h2><ul>
<li>WordNet 등의 시소러스를 이용하여 유의어를 얻거나, 단어사이의 유사도를 측정하는 등 유용한 작업을 할 수 있음</li>
<li>그러나 시소러스 기반 기법은 비용 문제나 새로운 단어에 대응이 어렵다는 문제가 있음</li>
</ul>
<h2 id="통계-기반-기법-Corpus"><a href="#통계-기반-기법-Corpus" class="headerlink" title="통계 기반 기법 - Corpus"></a>통계 기반 기법 - Corpus</h2><ul>
<li>그 결과 현재는 말뭉치(corpus)를 이용해 vectorization하는 방법을 주로 사용</li>
<li>분포 가설(distributional hypothesis) : ‘단어의 의미는 주변 단어에 의해 형성된다’는 가설</li>
</ul>
<h3 id="동시발생-행렬-co-occurence-matrix"><a href="#동시발생-행렬-co-occurence-matrix" class="headerlink" title="동시발생 행렬(co-occurence matrix)"></a>동시발생 행렬(co-occurence matrix)</h3><ul>
<li>각 말뭉치 안의 각 단어에 대해 주변 단어의 빈도를 집계</li>
<li>Cosine similarity로 단어 벡터간 유사도 계산</li>
</ul>
<h3 id="PPMI-Positive-Pointwise-Mutual-Information"><a href="#PPMI-Positive-Pointwise-Mutual-Information" class="headerlink" title="PPMI(Positive Pointwise Mutual Information)"></a>PPMI(Positive Pointwise Mutual Information)</h3><ul>
<li>PPMI : ‘a’, ‘the’ 처럼 단어와 관련성이 적지만 동시에 자주 발생하는 경우를 고려<ul>
<li>PPMI의 경우 차원수가 corpus의 어휘 수와 같으므로 차원수가 너무 커지는 문제가 있음</li>
<li>SVD를 통해 차원축소</li>
</ul>
</li>
<li>PTB dataset을 통해 테스트</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/25/sql/%EB%B9%85%EB%B6%84%EA%B8%B0-1-1-2%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/25/sql/%EB%B9%85%EB%B6%84%EA%B8%B0-1-1-2%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4/" class="post-title-link" itemprop="url">빅분기 1-1-2 Database</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2021-09-25T00:00:00+09:00">2021-09-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-09-26 21:11:21" itemprop="dateModified" datetime="2021-09-26T21:11:21+09:00">2021-09-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%EB%B9%85%EB%B6%84%EA%B8%B0-%ED%95%84%EA%B8%B0/" itemprop="url" rel="index"><span itemprop="name">빅분기 필기</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-1-2-데이터베이스"><a href="#1-1-2-데이터베이스" class="headerlink" title="1-1-2 데이터베이스"></a>1-1-2 데이터베이스</h1><h1 id="2-데이터베이스"><a href="#2-데이터베이스" class="headerlink" title="2. 데이터베이스"></a>2. 데이터베이스</h1><h3 id="데이터베이스-관리-시스템-DBMS-DataBase-Management-System"><a href="#데이터베이스-관리-시스템-DBMS-DataBase-Management-System" class="headerlink" title="데이터베이스 관리 시스템(DBMS: DataBase Management System)"></a>데이터베이스 관리 시스템(DBMS: DataBase Management System)</h3><ol>
<li>관계형 DBMS : 데이터를 열과 행을 이루는 테이블로 표현</li>
<li>객체지향 DBMS : 정보를 객체 형태로 표현</li>
<li>네트워크 DBMS : 그래프 구조를 기반으로 하는 모델</li>
<li>계층형 DMBS : 트리 구조를 기반으로 하는 모델</li>
</ol>
<h3 id="SQL-Structured-Query-Language"><a href="#SQL-Structured-Query-Language" class="headerlink" title="SQL (Structured Query Language)"></a>SQL (Structured Query Language)</h3><ul>
<li>DB에 접근할 때 사용하는 언어</li>
<li>데이터의 정의와 조작 기능을 가지고 있음</li>
<li>테이블 단위로 연산을 수행. 사용하기 쉬움</li>
</ul>
<h3 id="DB의-특징"><a href="#DB의-특징" class="headerlink" title="DB의 특징"></a>DB의 특징</h3><ol>
<li>통합된 데이터(Integrated Data)<ul>
<li>동일한 데이터가 중복되어 저장되지 않음을 의미</li>
<li>데이터의 중복은 관리상 복잡하고 다양한 문제를 초래함</li>
</ul>
</li>
<li>저장된 데이터(Stored Data)<ul>
<li>컴퓨터가 접근할 수 있는 저장매체에 데이터를 저장</li>
</ul>
</li>
<li>공용 데이터(Shared Data)<ul>
<li>여러 사용자가 서로 다른 목적으로 데이터를 함께 이용</li>
</ul>
</li>
<li>변화되는 데이터(Changed Data)<ul>
<li>지속적으로 갱신되면서도 현재의 정확한 데이터를 유지해야 함</li>
</ul>
</li>
</ol>
<p><a href="1-1-2%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%B3%206d578a437b4844228df6d417dcdc87df/DB%E1%84%8B%E1%85%B4%20%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B7%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A1%E1%84%87%E1%85%A9%E1%86%AB%2074bec8f976004579b4295aa517dc3099.csv">DB의 장단점</a></p>
<h3 id="DB의-활용"><a href="#DB의-활용" class="headerlink" title="DB의 활용"></a>DB의 활용</h3><ol>
<li>OLTP(OnLine Transacction Processing)</li>
</ol>
<ul>
<li>데이터 갱신 위주</li>
</ul>
<ol>
<li>OLAD(OnLine Analytical Precessing</li>
</ol>
<ul>
<li>데이터 조회 위주</li>
</ul>
<p><a href="1-1-2%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%B3%206d578a437b4844228df6d417dcdc87df/%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%B3%20a5e5a5be7ff34f538341c582656ce238.csv">제목 없음</a></p>
<h3 id="데이터-웨어하우스"><a href="#데이터-웨어하우스" class="headerlink" title="데이터 웨어하우스"></a>데이터 웨어하우스</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/21/sql/%EB%B9%85%EB%B6%84%EA%B8%B0-1-1-1-data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/21/sql/%EB%B9%85%EB%B6%84%EA%B8%B0-1-1-1-data/" class="post-title-link" itemprop="url">1-1 데이터와 정보</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-21 00:00:00" itemprop="dateCreated datePublished" datetime="2021-09-21T00:00:00+09:00">2021-09-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-01 23:38:59" itemprop="dateModified" datetime="2021-11-01T23:38:59+09:00">2021-11-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%EB%B9%85%EB%B6%84%EA%B8%B0-%ED%95%84%EA%B8%B0/" itemprop="url" rel="index"><span itemprop="name">빅분기 필기</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="빅데이터-개요-및-활용"><a href="#빅데이터-개요-및-활용" class="headerlink" title="빅데이터 개요 및 활용"></a>빅데이터 개요 및 활용</h1><h1 id="1-데이터와-정보"><a href="#1-데이터와-정보" class="headerlink" title="1. 데이터와 정보"></a>1. 데이터와 정보</h1><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><ul>
<li>데이터는 현실 세계에서 관찰/측정하여 수집하한 사실로 추론과 추정의 근거를 이룸</li>
<li>단순한 객체로도 가치가 있으며, 다른 객체와의 상호관계 속에서 더 큰 가치를 가짐</li>
<li>객관적 사실이라는 존재적 특성</li>
<li>추론, 추정, 예측, 전망을 위한 근거로써 당위적 특성을 가짐</li>
</ul>
<h3 id="데이터의-구분"><a href="#데이터의-구분" class="headerlink" title="데이터의 구분"></a>데이터의 구분</h3><ol>
<li><p>정량적 데이터(Quantitative Data)</p>
<ul>
<li>주로 숫자로 이루어진 데이터</li>
<li>Ex) 2021년, 100km/h 등</li>
</ul>
</li>
<li><p>정성적 데이터(Qualitative Data)</p>
<ul>
<li>문자와 같은 텍스트로 구성, 함축적 의미를 지닌 데이터</li>
<li>ex) 철수가 시험에 합격하였다.</li>
</ul>
</li>
</ol>
<h3 id="데이터의-유형"><a href="#데이터의-유형" class="headerlink" title="데이터의 유형"></a>데이터의 유형</h3><ol>
<li><p>정형 데이터(Structured Data)</p>
<ul>
<li>정해진 형식과 구조에 맞게 저장</li>
<li>연산이 가능함</li>
<li>Ex) 관계형 DB의 테이블에 저장되는 데이터</li>
</ul>
</li>
<li><p>반정형 데이터(Semi-structured Data)</p>
<ul>
<li>데이터의 형식과 구조가 비교적 유연함</li>
<li>스키마 정보를 데이터와 함께 제공하는 형식의 데이터</li>
<li>연산이 불가능</li>
<li>Ex) JSON, XML, RDF, HTML 등</li>
</ul>
</li>
<li><p>비정형 데이터(Unstructured Data)</p>
<ul>
<li>구조가 정해지지 않은 대부분의 데이터</li>
<li>연산이 불가능</li>
<li>Ex) 동영상, 이미지, 음성, 문서, 메일 등</li>
</ul>
</li>
</ol>
<p>[정량적 vs 정성적]</p>
<h3 id="데이터의-근원에-따른-분류"><a href="#데이터의-근원에-따른-분류" class="headerlink" title="데이터의 근원에 따른 분류"></a>데이터의 근원에 따른 분류</h3><ol>
<li><p>가역 데이터</p>
<ul>
<li>데이터의 원본으로 일정 수준 환원이 가능한 데이터</li>
<li>이력 추적이 가능, 원본 데이터의 변경사항 반영 가능</li>
</ul>
</li>
<li><p>불가역 데이터</p>
<ul>
<li>원본으로 환원이 불가능한 데이터</li>
<li><p>원본 데이터와는 전혀 다른 형태로 재생산, 변경사항 반영 불가</p>
<p><a target="_blank" rel="noopener" href="https://www.notion.so/59ba84f58c0445fdbaa31a0948f01704">가역 vs 불가역</a></p>
</li>
</ul>
</li>
</ol>
<h3 id="데이터의-기능"><a href="#데이터의-기능" class="headerlink" title="데이터의 기능"></a>데이터의 기능</h3><p>데이터를 기반으로 한 암묵지와 형식지의 상호작용이 중요</p>
<ol>
<li><p>암묵지</p>
<ul>
<li>어떠한 시행착오나 경험을 통해 개인에게 체계화 된 지식</li>
<li>겉으로 드러나지 않고, 전달과 공유가 어려움</li>
</ul>
</li>
<li><p>형식지</p>
<ul>
<li>형상화된 유형의 지식</li>
<li>전달과 공유가 쉬움</li>
</ul>
</li>
</ol>
<h3 id="지식-창조-매커니즘"><a href="#지식-창조-매커니즘" class="headerlink" title="지식 창조 매커니즘"></a>지식 창조 매커니즘</h3><p>암묵지와 형식지 간 상호작용을 위한 지식창조 매커니즘</p>
<ol>
<li><p>공통화(Socialization)</p>
<ul>
<li>서로의 경험이나 인식을 공유하며 한 차원 높은 암묵지로 발전</li>
</ul>
</li>
<li><p>표출화(Externalization)</p>
<ul>
<li>암묵지가 구체화되어 외부(형식지)로 표현</li>
</ul>
</li>
<li><p>연결화(Combination)</p>
<ul>
<li>형식지를 재분류하여 체계화</li>
</ul>
</li>
<li><p>내면화(Internalization)</p>
<ul>
<li>전달받은 형식지를 다시 개인의 것으로 만듬</li>
</ul>
</li>
</ol>
<p>공통화 → 표출화 → 연결화 → 내면화 → 공통화 ———&gt; ….</p>
<h3 id="데이터-정보-지식-지혜"><a href="#데이터-정보-지식-지혜" class="headerlink" title="데이터, 정보, 지식 지혜"></a>데이터, 정보, 지식 지혜</h3><p>지식의 피라미드 : 데이터 → 정보 → 지식 → 지혜</p>
<ol>
<li><p>데이터(Data) </p>
<ul>
<li>현실에서 관찰하거나 측정하여 수집한 사실이나 값 / 객관적 사실</li>
<li>Ex) 온라인 쇼핑 시 노트북 가격은 100만원이며, 오프라인에서는 150만원이다.</li>
</ul>
</li>
<li><p>정보(Information) </p>
<ul>
<li>데이터를 가공하거나 처리하여 도출한 의미</li>
<li>Ex) 오프라인 상점보다 온라인 쇼핑 시 노트북 가격이 더 저렴하다.</li>
</ul>
</li>
<li><p>지식(Knowledge)</p>
<ul>
<li>상호 연결된 정보를 구조화하여 유의미한 정보를 분류</li>
<li>개인적인 경험을 결합시켜 내재화한 고유의 결과물</li>
<li>Ex) 오프라인 보다 저렴한 온라인으로 노트북을 구매할 것이다</li>
</ul>
</li>
<li><p>지혜(Wisdom)</p>
<ul>
<li>축적된 지식을 통해 근본적인 원리를 이해하고 아이디어를 결합하여 도출한 창의적 산물</li>
<li>Ex) 다른 상품들도 온라인 쇼핑 시 오프라인보다 저렴할 것이다.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/30/sql/SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/30/sql/SQL/" class="post-title-link" itemprop="url">SQL-01</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-30 00:00:00" itemprop="dateCreated datePublished" datetime="2021-08-30T00:00:00+09:00">2021-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-10 14:52:37" itemprop="dateModified" datetime="2021-10-10T14:52:37+09:00">2021-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/SQL/" itemprop="url" rel="index"><span itemprop="name">SQL</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SQL-1"><a href="#SQL-1" class="headerlink" title="SQL-1"></a>SQL-1</h1><h3 id="DB-Database"><a href="#DB-Database" class="headerlink" title="DB : Database"></a>DB : Database</h3><p>데이터를 통합하여 관리하는 데이터의 집합</p>
<h3 id="DBMS-Database-Management-System"><a href="#DBMS-Database-Management-System" class="headerlink" title="DBMS(Database Management System)"></a>DBMS(Database Management System)</h3><p>데이터 베이스를 관리하는 시스템</p>
<h3 id="RDBMS-Relational-Database-Management-System"><a href="#RDBMS-Relational-Database-Management-System" class="headerlink" title="RDBMS : Relational Database Management System"></a>RDBMS : Relational Database Management System</h3><p>Oracle, Mysql, Postgresql, Sqlite</p>
<p>데이터의 테이블 사이에 키값으로 관계를 가지고 있는 데이터베이스</p>
<h3 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h3><p>Mongodb, Hbase, Cassandra</p>
<p>데이터 테이블 사이의 관계가 없이 데이터를 저장하는 DB.</p>
<p>데이터 사이의 관계가 없으므로 복잡성이 적고 많은 데이터의 저장이 가능.</p>
<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="특징"><a href="#특징" class="headerlink" title="특징"></a>특징</h3><p>MySQL은 오픈소스이며, 다중사용자와 다중 스레드 지원.</p>
<p>다양한 운영체제에 다양한 프로그래밍 언어 지원.</p>
<p>표준 SQL을 사용.</p>
<p>작고 강력하며 가격이 저렴.</p>
<p>AWS 우분투 연결</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~Downloads/ssac.pem ubuntu@ip</span><br></pre></td></tr></table></figure>
<p>인스턴스 생성 시 받았던 비밀키랑, 퍼블릭 IP 사용</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update -y</span><br><span class="line">sudo apt-get upgrade -y</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y mysql-server mysql-client</span><br><span class="line">sudo mysql</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>,authentication_string,plugin,host <span class="keyword">FROM</span> mysql.user;</span><br><span class="line">##비밀번호를 pwd로 변경(내부 패스워드 로컬)</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">USER</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> IDENTIFIED <span class="keyword">WITH</span> mysql_native_password <span class="keyword">BY</span> <span class="string">&#x27;pwd&#x27;</span>;</span><br><span class="line"></span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="keyword">user</span>,authentication_string,plugin,host <span class="keyword">FROM</span> mysql.user;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ubuntu에서 이렇게 하고 패스워드 쳐서 mysql 접속</span></span><br><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line"><span class="comment">#두번째 패스워드 생성//외부접속 패스워드, 인터넷으로 접속 할 때</span></span><br><span class="line">sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br><span class="line"></span><br><span class="line"><span class="comment">#vim에서 /bind로 bind 검색 가능, shift-d 하면 </span></span><br><span class="line">커서 뒤 다 삭제</span><br><span class="line">여기서 bind-adress를 127.0.0.1을 0.0.0.0으로 변경</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#외부접속 패스워드 생성</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> root@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;pwd&#x27;</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart mysql</span><br></pre></td></tr></table></figure>
<p>(PC)work bench(wb) → Internet → MySQL(server)</p>
<p>필요한 것</p>
<p>public ip, password</p>
<p>MySQL Workvench에서 MySQL Connection 옆에 + 버튼 누르고</p>
<p>Setup 에서 Connection Name 작성(SSAC)</p>
<p>Hostname(public IP), username 입력, PW는 아까 햇던 ssac</p>
<p>이때 username은 ubuntu가 아니라 root. mySQL 서버에 접속하는거이기 때문.</p>
<p>FIle → OPen SQL script 해서 데이터 불러오고</p>
<p>File → new model</p>
<p>주석</p>
<p>/<em> ㅇㅇㅇㅇ </em></p>
<p>— 한줄</p>
<p>`# &lt;- 얘도 workbench에서 지원해줌</p>
<p>heidiSQL → 가벼움(윈도우</p>
<p>Seuqelpro → 맥에서 가볍게 쓸때</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dongyoung Kim</p>
  <div class="site-description" itemprop="description">Mathematics, Python, ML, Data</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dongyoung0" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dongyoung0" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wmkimdy@gmail.com" title="E-Mail → mailto:wmkimdy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongyoung Kim</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
