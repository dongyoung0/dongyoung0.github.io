<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Rnnlm Generator 언어 모델(RNNLM)을 사용하여 문장 생성을 수행. 말뭉치를 사용해 학습한 언어 모델을 사용하여 새로운 문장을 만들어 냄 그 다음 개선된 언어 모델을 이용하여 더 자연스러운 문장을 생성 seq2seq : sequence(시계열 데이터)를 다른 sequence로 변환   deterministic :  특정 단어 뒤에 나올 확률이">
<meta property="og:type" content="article">
<meta property="og:title" content="Ch 07. seq2seq">
<meta property="og:url" content="http://example.com/2021/10/28/nlp/from-scratch/ch07_seq2seq/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Rnnlm Generator 언어 모델(RNNLM)을 사용하여 문장 생성을 수행. 말뭉치를 사용해 학습한 언어 모델을 사용하여 새로운 문장을 만들어 냄 그 다음 개선된 언어 모델을 이용하여 더 자연스러운 문장을 생성 seq2seq : sequence(시계열 데이터)를 다른 sequence로 변환   deterministic :  특정 단어 뒤에 나올 확률이">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/fig%207-10.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/fig%207-19.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/out%207-1.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/fig%207-23.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/out%207-2.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/fig%207-26.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/out%207-3.png">
<meta property="og:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/out%207-4.png">
<meta property="article:published_time" content="2021-10-27T15:00:00.000Z">
<meta property="article:modified_time" content="2021-11-10T07:32:36.389Z">
<meta property="article:author" content="Dongyoung Kim">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="seq2seq">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/10/28/nlp/from-scratch/image/nlp-from-scratch/fig%207-10.png">

<link rel="canonical" href="http://example.com/2021/10/28/nlp/from-scratch/ch07_seq2seq/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Ch 07. seq2seq | Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">공부한 내용을 정리하는 장소</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">10</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">9</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/28/nlp/from-scratch/ch07_seq2seq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ch 07. seq2seq<a href="https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name_posts/nlp/from-scratch/ch07_seq2seq.md" class="post-edit-link" title="Edit this post" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-28 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-28T00:00:00+09:00">2021-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-10 16:32:36" itemprop="dateModified" datetime="2021-11-10T16:32:36+09:00">2021-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Rnnlm-Generator"><a href="#Rnnlm-Generator" class="headerlink" title="Rnnlm Generator"></a>Rnnlm Generator</h2><ul>
<li>언어 모델(RNNLM)을 사용하여 문장 생성을 수행.</li>
<li>말뭉치를 사용해 학습한 언어 모델을 사용하여 새로운 문장을 만들어 냄</li>
<li>그 다음 개선된 언어 모델을 이용하여 더 자연스러운 문장을 생성</li>
<li>seq2seq : sequence(시계열 데이터)를 다른 sequence로 변환</li>
</ul>
<ul>
<li>deterministic :  특정 단어 뒤에 나올 확률이 가장 높은 단어를 선택</li>
<li>probabilistic : 확률적으로 다음 단어를 샘플링</li>
<li>위 과정을 종결 기호가 나타날때 까지 반복하여 새로운 문장 생성</li>
</ul>
<h4 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnlmGen</span>(<span class="params">Rnnlm</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, start_id, skip_ids=<span class="literal">None</span>, sample_size=<span class="number">100</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rnnlm을 이용하여 문장 생성.</span></span><br><span class="line"><span class="string">        start_id : 첫 단어의 ID</span></span><br><span class="line"><span class="string">        skip_ids : 샘플링되지 않을 단어를 지정해주는 리스트 </span></span><br><span class="line"><span class="string">        (예를 들어 PTB data set에서 &lt;unk&gt; : 희소한 단어, N : 숫자 등을 제외)</span></span><br><span class="line"><span class="string">        sample_size : 샘플링 할 단어의 수</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        word_ids = [start_id]</span><br><span class="line">        </span><br><span class="line">        x = start_id</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(word_ids) &lt; sample_size:</span><br><span class="line">            x = np.array(x).reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">            score = self.predict(x)</span><br><span class="line">            p = softmax(score.flatten())</span><br><span class="line">            </span><br><span class="line">            sampled = np.random.choice(<span class="built_in">len</span>(p), size=<span class="number">1</span>, p=p)</span><br><span class="line">            <span class="keyword">if</span> (skip_ids <span class="keyword">is</span> <span class="literal">None</span>) <span class="keyword">or</span> (sampled <span class="keyword">not</span> <span class="keyword">in</span> skip_ids):</span><br><span class="line">                x = sampled</span><br><span class="line">                word_ids.append(<span class="built_in">int</span>(x))</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> word_ids</span><br></pre></td></tr></table></figure>
<h4 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h4><ul>
<li>위 구현을 사용하여 ‘you’로 시작하는 문장 생성해보기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">corpus_size = <span class="built_in">len</span>(corpus)</span><br><span class="line"></span><br><span class="line">model = RnnlmGen()</span><br><span class="line"><span class="comment"># model.load_params(../practice/Rnnlm.pkl)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><pre><code>you lenses beings wishes donoghue violin hourly listen manage chair trinova bergsma reopen essentially ann fan phased e two-year middlemen darman retail dealers open running quota managed hill stem lighting claims incorporated professor grid tough nor honecker apparently livestock wellington broker-dealer bobby shareholders guterman willful old-fashioned such heels does fearful donald dominates which assembly very genetically region packwood establishing divisive suburb slow am cars gatt draws unlawful reacting emerging technologies animals johnson applied treat hurdle network beleaguered reformers commonwealth hurdles transactions broderick which unesco watch block careful builds walked prevailed grew pittsburgh responsibility bets suits highlight slip liquidate exterior experts
</code></pre><h4 id="6장에서-학습시켰던-모델을-사용해-문장-생성"><a href="#6장에서-학습시켰던-모델을-사용해-문장-생성" class="headerlink" title="6장에서 학습시켰던 모델을 사용해 문장 생성"></a>6장에서 학습시켰던 모델을 사용해 문장 생성</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model = RnnlmGen()</span></span><br><span class="line">model.load_params(<span class="string">&#x27;../practice/Rnnlm.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<h4 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h4><pre><code>you retain evaluation such bad municipal managers and constitute the written mood.
 he is to be at john salinas calif. resigned.
 mr. roman was among a share of mr. simmons and disclosed the federal court threw by the fund because they would n&#39;t retreated the agreement to assist its two uv-b advice.
 joining these reviews resources on county new jersey comparison that warner was out it too.
 the west german news is increasingly buying seriously to bankers and lincoln soon to make up their own troubles.
 mr. stone canada the chapter year lacks he had
</code></pre><h3 id="Better-RnnlmGen"><a href="#Better-RnnlmGen" class="headerlink" title="Better RnnlmGen"></a>Better RnnlmGen</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BetterRnnlmGen</span>(<span class="params">BetterRnnlm</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, start_id, skip_ids=<span class="literal">None</span>, sample_size=<span class="number">100</span></span>):</span></span><br><span class="line">        word_ids = [start_id]</span><br><span class="line"></span><br><span class="line">        x = start_id</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(word_ids) &lt; sample_size:</span><br><span class="line">            x = np.array(x).reshape(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            score = self.predict(x).flatten()</span><br><span class="line">            p = softmax(score).flatten()</span><br><span class="line"></span><br><span class="line">            sampled = np.random.choice(<span class="built_in">len</span>(p), size=<span class="number">1</span>, p=p)</span><br><span class="line">            <span class="keyword">if</span> (skip_ids <span class="keyword">is</span> <span class="literal">None</span>) <span class="keyword">or</span> (sampled <span class="keyword">not</span> <span class="keyword">in</span> skip_ids):</span><br><span class="line">                x = sampled</span><br><span class="line">                word_ids.append(<span class="built_in">int</span>(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        states = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.lstm_layers:</span><br><span class="line">            states.append((layer.h, layer.c))</span><br><span class="line">        <span class="keyword">return</span> states</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span>(<span class="params">self, states</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer, state <span class="keyword">in</span> <span class="built_in">zip</span>(self.lstm_layers, states):</span><br><span class="line">            layer.set_state(*state)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlmGen()</span><br><span class="line"><span class="comment">#model.load_params(&#x27;../practice/Rnnlm.pkl&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>you modestly mead methods tenn. chlorofluorocarbons photographic cypress substance medicare consequences deny via modified mph welch followed spurred determined failures lawsuits poland w. making prepares machinists yields pakistan sugar assault impending editor unfavorable implied priority we purchase coupons flagship tree hang smiling bridge duck joe agreement lighter release deadlines or drivers checks expenses recall decides pipelines movies practiced equity exhibit financially lawsuit reality so-called lesser press automotive springs restoration workplace continually calendar ai fighter dealing hourly ivory arnold suitable mentality improved j.c. designing procedures trader foundations kerry sank strategic show reference fame chaotic competition very fha reading releases minor stretching
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlmGen()</span><br><span class="line">model.load_params(<span class="string">&#x27;../code/rnnlm/BetterRnnlm.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 시작 문자 및 skip words 설정</span></span><br><span class="line">start_word = <span class="string">&#x27;you&#x27;</span></span><br><span class="line">start_id = word_to_id[start_word]</span><br><span class="line">skip_words = [<span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;$&#x27;</span>]</span><br><span class="line">skip_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> skip_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 문장 생성</span></span><br><span class="line">word_ids = model.generate(start_id, skip_ids)</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>you did n&#39;t know how a spokesman said in no matter.
 they was very skeptical of the lone star &#39;s line in union by a worth.
 for that time it will take payment of the large market for salespeople.
 a ministry official spent a hat regarding the new state record against a led to the national association of journal and financial services the olympic office of science which had begun a wall breaker palace and the orange workers.
 in his wake in june she cautioned the company &#39;s defense portfolio would be selling about the third
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model.reset_state()</span><br><span class="line"></span><br><span class="line">start_words = <span class="string">&#x27;the meaning of life is&#x27;</span></span><br><span class="line">start_ids = [word_to_id[w] <span class="keyword">for</span> w <span class="keyword">in</span> start_words.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> start_ids[:-<span class="number">1</span>]:</span><br><span class="line">    x = np.array(x).reshape(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    model.predict(x)</span><br><span class="line"></span><br><span class="line">word_ids = model.generate(start_ids[-<span class="number">1</span>], skip_ids)</span><br><span class="line">word_ids = start_ids[:-<span class="number">1</span>] + word_ids</span><br><span class="line">txt = <span class="string">&#x27; &#x27;</span>.join([id_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids])</span><br><span class="line">txt = txt.replace(<span class="string">&#x27; &lt;eos&gt;&#x27;</span>, <span class="string">&#x27;.\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(txt)</span><br></pre></td></tr></table></figure>
<pre><code>--------------------------------------------------
the meaning of life is a while it could harm the problem of the trigger to make those difficulties in an adversary treaty are such chemistry as the nation &#39;s defense opposition.
 the prosecutor would assume that if that gets business money would otherwise make it difficult for its future calculations.
 if people needed money like good research the need for a one-hour champion to finance techniques.
 white activists and consultants wo n&#39;t go good until most say they did n&#39;t know deals with a different class but they still are simply considering a death and carry sent big-time tax benefits
</code></pre><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><ul>
<li>Encoder - Decoder : LSTM 2개로 구성</li>
<li>Encoder가 문장을 hidden state $h$로 변환시키면</li>
<li>Decoder가 $h$를 입력으로 받아서 문장을 생성</li>
</ul>
<h3 id="toy-problem"><a href="#toy-problem" class="headerlink" title="toy problem"></a>toy problem</h3><ul>
<li><p>덧셈 문제</p>
<p>  <img src="../image/nlp-from-scratch/fig%207-10.png" alt="7-10"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&#x27;..&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> sequence</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>([id_to_char[c] <span class="keyword">for</span> c <span class="keyword">in</span> x_train[<span class="number">0</span>]], [id_to_char[c] <span class="keyword">for</span> c <span class="keyword">in</span> t_train[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;7&#39;, &#39;1&#39;, &#39;+&#39;, &#39;1&#39;, &#39;1&#39;, &#39;8&#39;, &#39; &#39;] [&#39;_&#39;, &#39;1&#39;, &#39;8&#39;, &#39;9&#39;, &#39; &#39;]
</code></pre><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>문자열을 입력받아서 벡터 $h$로 변환</li>
<li>LSTM을 사용하는 경우 hidden state $h$만 Decoder로 전달.<br>  cell $c$는 LSTM 자기 자신만 사용한다는 전제로 만들어졌기 때문.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        vocab_size : 어휘 수</span></span><br><span class="line"><span class="string">        wordvec_size : word vector의 dimension</span></span><br><span class="line"><span class="string">        hidden_size : LSTM layer의 hidden state vector의 dimension</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span>*H)/np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span>*H)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span>*H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.params = self.embed.params + self.lstm.params</span><br><span class="line">        self.grads = self.embed.grads + self.lstm.grads</span><br><span class="line">        self.hs = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        xs = self.embed.forward(xs)</span><br><span class="line">        hs = self.lstm.forward(xs)</span><br><span class="line">        self.hs = hs</span><br><span class="line">        <span class="keyword">return</span> hs[:,-<span class="number">1</span>,:]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dh</span>):</span></span><br><span class="line">        dhs = np.zeros_like(self.hs)</span><br><span class="line">        dhs[:, -<span class="number">1</span>, :] = dh</span><br><span class="line">        </span><br><span class="line">        dout = self.lstm.backward(dhs)</span><br><span class="line">        dout = self.embed.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br></pre></td></tr></table></figure>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li><p>Encoder가 출력한 $h$를 입력받아 목적으로 하는 다른 문자열을 출력</p>
</li>
<li><p>RNN으로 문장을 생성할 때, 학습 시에는 정답을 알고 있기 때문에 sequence를 한번에 입력.<br>하지만 문장을 생성할 때는 시작을 알리는 구분문자(‘_’)를 입력하고 다음 출력 문자를 입력으로 반복</p>
</li>
<li><p>위 덧셈 문제의 경우 답이 정해져 있는 문제기 때문에 deterministic하게(점수가 가장 높은 문자 고르기) 문자열 생성해 봄</p>
<p><img src="../image/nlp-from-scratch/fig%207-19.png" alt="7-19"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span>*H)/np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span>*H)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span>*H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(H,V)/np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>)</span><br><span class="line">        self.affine = TimeAffine(affine_W, affine_b)</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> (self.embed, self.lstm, self.affine):</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, h</span>):</span></span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        out = self.embed.forward(xs)</span><br><span class="line">        out = self.lstm.forward(out)</span><br><span class="line">        score = self.affine.forward(out)</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dscore</span>):</span></span><br><span class="line">        dout = self.affine.backward(dscore)</span><br><span class="line">        dout = self.lstm.backward(dout)</span><br><span class="line">        dout = self.embed.backward(dout)</span><br><span class="line">        dh = self.lstm.dh</span><br><span class="line">        <span class="keyword">return</span> dh</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, h, start_id, sample_size</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        위 forward는 학습 시 사용</span></span><br><span class="line"><span class="string">        generate는 새 문장을 생성할 때 사용</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        sampled = []</span><br><span class="line">        sample_id = start_id</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size):</span><br><span class="line">            x = np.array(sample_id).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            out = self.embed.forward(x)</span><br><span class="line">            out = self.lstm.forward(out)</span><br><span class="line">            score = self.affine.forward(out)</span><br><span class="line">            </span><br><span class="line">            sample_id = np.argmax(score.flatten())</span><br><span class="line">            sampled.append(<span class="built_in">int</span>(sample_id))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> sampled</span><br></pre></td></tr></table></figure>
<h3 id="Seq2seq-1"><a href="#Seq2seq-1" class="headerlink" title="Seq2seq"></a>Seq2seq</h3><ul>
<li>Encoder와 Decoder를 연결 후 Time Softmax with Loss를 통해 Loss 계산</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2seq</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        self.encoder = Encoder(V, D, H)</span><br><span class="line">        self.decoder = Decoder(V, D, H)</span><br><span class="line">        self.softmax = TimeSoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        self.params = self.encoder.params + self.decoder.params</span><br><span class="line">        self.grads = self.encoder.grads + self.decoder.grads</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        decoder_xs, decoder_ts = ts[:, :-<span class="number">1</span>], ts[:, <span class="number">1</span>:]</span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        score = self.decoder.forward(decoder_xs, h)</span><br><span class="line">        loss = self.softmax.forward(score, decoder_ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.softmax.backward(dout)</span><br><span class="line">        dh = self.decoder.backward(dout)</span><br><span class="line">        dout = self.encoder.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, xs, start_id, sample_size</span>):</span></span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        sampled = self.decoder.generate(h, start_id, sample_size)</span><br><span class="line">        <span class="keyword">return</span> sampled</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="덧셈-문제"><a href="#덧셈-문제" class="headerlink" title="덧셈 문제"></a>덧셈 문제</h3><ol>
<li>train data에서 mini-batch 선택</li>
<li>기울기 계산</li>
<li>parameter 갱신</li>
</ol>
<h4 id="Seq2seq-Evaluation-구현"><a href="#Seq2seq-Evaluation-구현" class="headerlink" title="Seq2seq Evaluation 구현"></a>Seq2seq Evaluation 구현</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_seq2seq</span>(<span class="params">model, question, correct, id_to_char,</span></span></span><br><span class="line"><span class="params"><span class="function">                 verbos=<span class="literal">False</span>, is_reverse=<span class="literal">False</span></span>):</span></span><br><span class="line">    correct = correct.flatten()</span><br><span class="line">    <span class="comment"># 머릿글자</span></span><br><span class="line">    start_id = correct[<span class="number">0</span>]</span><br><span class="line">    correct = correct[<span class="number">1</span>:]</span><br><span class="line">    guess = model.generate(question, start_id, <span class="built_in">len</span>(correct))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 문자열로 변환</span></span><br><span class="line">    question = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> question.flatten()])</span><br><span class="line">    correct = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> correct])</span><br><span class="line">    guess = <span class="string">&#x27;&#x27;</span>.join([id_to_char[<span class="built_in">int</span>(c)] <span class="keyword">for</span> c <span class="keyword">in</span> guess])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbos:</span><br><span class="line">        <span class="keyword">if</span> is_reverse:</span><br><span class="line">            question = question[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        colors = &#123;<span class="string">&#x27;ok&#x27;</span>: <span class="string">&#x27;\033[92m&#x27;</span>, <span class="string">&#x27;fail&#x27;</span>: <span class="string">&#x27;\033[91m&#x27;</span>, <span class="string">&#x27;close&#x27;</span>: <span class="string">&#x27;\033[0m&#x27;</span>&#125;</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Q&#x27;</span>, question)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;T&#x27;</span>, correct)</span><br><span class="line"></span><br><span class="line">        is_windows = os.name == <span class="string">&#x27;nt&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> correct == guess:</span><br><span class="line">            mark = colors[<span class="string">&#x27;ok&#x27;</span>] + <span class="string">&#x27;☑&#x27;</span> + colors[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> is_windows:</span><br><span class="line">                mark = <span class="string">&#x27;O&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(mark + <span class="string">&#x27; &#x27;</span> + guess)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark = colors[<span class="string">&#x27;fail&#x27;</span>] + <span class="string">&#x27;☒&#x27;</span> + colors[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> is_windows:</span><br><span class="line">                mark = <span class="string">&#x27;X&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(mark + <span class="string">&#x27; &#x27;</span> + guess)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;---&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> guess == correct <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h4 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line">model = Seq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-1.png" alt="out1"></p>
<h2 id="seq2seq-개선"><a href="#seq2seq-개선" class="headerlink" title="seq2seq 개선"></a>seq2seq 개선</h2><h3 id="Reverse"><a href="#Reverse" class="headerlink" title="Reverse"></a>Reverse</h3><ul>
<li>입력 데이터의 순서를 반전 시키면 학습 진행속도도 빨라지고, 정확도도 좋아짐<br><img src="../image/nlp-from-scratch/fig%207-23.png" alt="7-23"> </li>
<li>직관적으로 ‘나는 고양이로소이다’를 ‘I am a cat’로 번역하는 문제에서 back-propagation시<br>[‘이다’, ‘로소’, ‘고양이’, ‘는’, ‘나’]로 반전시키면 ‘나’와 ‘I’가 붙어있어서 기울기가 잘 전달 됨</li>
<li>입력 데이터를 반전시켜도 단어 사이의 ‘평균 거리’는 그대로임.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">x_train, x_test = x_train[:, ::-<span class="number">1</span>], x_test[:, ::-<span class="number">1</span>]</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = Seq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_rev = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_rev.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-2.png" alt="out2"></p>
<h3 id="Peeky-Decoder"><a href="#Peeky-Decoder" class="headerlink" title="Peeky Decoder"></a>Peeky Decoder</h3><ul>
<li>기존 seq2seq에서는 ‘입력 문장’을 Encoder가 $h$로 변환하여 Decoder에 전달하고<br>Decoder의 최초 시점의 LSTM layer에서만 $h$를 이용함</li>
<li>중요 정보인 $h$를 Decoder의 여러 layer에서 사용하는 것이 Peeky Decoder<br><img src="../image/nlp-from-scratch/fig%207-26.png" alt="7-26"></li>
<li>Decoder의 모든 시각($t$)의 Affine, LSTM layer에 $h$를 전달<br>여러 layer가 중요 정보인 $h$를 공유함으로서 더 올바른 결정을 내림</li>
<li>$h$에 대한 계산을 추가로 해줘야 하므로 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeekyDecoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        <span class="comment"># hidden 추가</span></span><br><span class="line">        lstm_Wx = (rn(H + D, <span class="number">4</span> * H) / np.sqrt(H + D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        <span class="comment">#hidden 추가</span></span><br><span class="line">        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.embed = TimeEmbedding(embed_W)</span><br><span class="line">        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>)</span><br><span class="line">        self.affine = TimeAffine(affine_W, affine_b)</span><br><span class="line">        </span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> (self.embed, self.lstm, self.affine):</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, h</span>):</span></span><br><span class="line">        N, T = xs.shape</span><br><span class="line">        N, H = h.shape</span><br><span class="line">        </span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line">        </span><br><span class="line">        out = self.embed.forward(xs)</span><br><span class="line">        <span class="comment"># 추가된 hidden h와 기존에 전달되던 out을 concat</span></span><br><span class="line">        hs = np.repeat(h, T, axis=<span class="number">0</span>).reshape(N, T, H)</span><br><span class="line">        out = np.concatenate((hs, out), axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        out = self.lstm.forward(out)</span><br><span class="line">        <span class="comment"># 위와 마찬가지로 h 추가</span></span><br><span class="line">        out = np.concatenate((hs, out), axis=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        score = self.affine.forward(out)</span><br><span class="line">        self.cache = H</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dscore</span>):</span></span><br><span class="line">        H = self.cache</span><br><span class="line"></span><br><span class="line">        dout = self.affine.backward(dscore)</span><br><span class="line">        <span class="comment"># 추가된 h 반영</span></span><br><span class="line">        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]</span><br><span class="line">        dout = self.lstm.backward(dout)</span><br><span class="line">        <span class="comment"># 추가된 h 반영2</span></span><br><span class="line">        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]</span><br><span class="line">        self.embed.backward(dembed)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 추가된 h 반영3</span></span><br><span class="line">        dhs = dhs0 + dhs1</span><br><span class="line">        dh = self.lstm.dh + np.<span class="built_in">sum</span>(dhs, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dh</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, h, start_id, sample_size</span>):</span></span><br><span class="line">        sampled = []</span><br><span class="line">        char_id = start_id</span><br><span class="line">        self.lstm.set_state(h)</span><br><span class="line"></span><br><span class="line">        H = h.shape[<span class="number">1</span>]</span><br><span class="line">        peeky_h = h.reshape(<span class="number">1</span>, <span class="number">1</span>, H)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size):</span><br><span class="line">            x = np.array([char_id]).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            out = self.embed.forward(x)</span><br><span class="line"></span><br><span class="line">            out = np.concatenate((peeky_h, out), axis=<span class="number">2</span>)</span><br><span class="line">            out = self.lstm.forward(out)</span><br><span class="line">            out = np.concatenate((peeky_h, out), axis=<span class="number">2</span>)</span><br><span class="line">            score = self.affine.forward(out)</span><br><span class="line"></span><br><span class="line">            char_id = np.argmax(score.flatten())</span><br><span class="line">            sampled.append(char_id)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sampled        </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeekySeq2seq</span>(<span class="params">Seq2seq</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, wordvec_size, hidden_size</span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        self.encoder = Encoder(V, D, H)</span><br><span class="line">        self.decoder = PeekyDecoder(V, D, H)</span><br><span class="line">        self.softmax = TimeSoftmaxWithLoss()</span><br><span class="line">        </span><br><span class="line">        self.params = self.encoder.params + self.decoder.params</span><br><span class="line">        self.grads = self.encoder.grads + self.decoder.grads</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        decoder_xs, decoder_ts = ts[:, :-<span class="number">1</span>], ts[:, <span class="number">1</span>:]</span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        score = self.decoder.forward(decoder_xs, h)</span><br><span class="line">        loss = self.softmax.forward(score, decoder_ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.softmax.backward(dout)</span><br><span class="line">        dh = self.decoder.backward(dout)</span><br><span class="line">        dout = self.encoder.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, xs, start_id, sample_size</span>):</span></span><br><span class="line">        h = self.encoder.forward(xs)</span><br><span class="line">        sampled = self.decoder.generate(h, start_id, sample_size)</span><br><span class="line">        <span class="keyword">return</span> sampled</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="Peeky-seq2seq-학습"><a href="#Peeky-seq2seq-학습" class="headerlink" title="Peeky seq2seq 학습"></a>Peeky seq2seq 학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line"><span class="comment">#x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]</span></span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_peeky = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_peeky.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-3.png" alt="out3"></p>
<h3 id="Peeky-seq2seq-reversed-학습"><a href="#Peeky-seq2seq-reversed-학습" class="headerlink" title="Peeky seq2seq(+ reversed) 학습"></a>Peeky seq2seq(+ reversed) 학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = sequence.load_data(<span class="string">&#x27;addition.txt&#x27;</span>, seed=<span class="number">1984</span>)</span><br><span class="line">x_train, x_test = x_train[:, ::-<span class="number">1</span>], x_test[:, ::-<span class="number">1</span>]</span><br><span class="line">char_to_id, id_to_char = sequence.get_vocab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter setting</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(char_to_id)</span><br><span class="line">wordvec_size = <span class="number">16</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">max_epoch = <span class="number">25</span></span><br><span class="line">max_grad = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = Adam()</span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">acc_list_peeky_rev = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    trainer.fit(x_train, t_train, max_epoch=<span class="number">1</span>, batch_size=batch_size, max_grad=max_grad)</span><br><span class="line">    </span><br><span class="line">    correct_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_test)):</span><br><span class="line">        q, ans = x_test[[i]], t_test[[i]]</span><br><span class="line">        verbose = i &lt; <span class="number">10</span></span><br><span class="line">        correct_num += eval_seq2seq(model, q, ans, id_to_char, verbose)</span><br><span class="line">        </span><br><span class="line">    acc = <span class="built_in">float</span>(correct_num) / <span class="built_in">len</span>(x_test)</span><br><span class="line">    acc_list_peeky_rev.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;accuracy : <span class="subst">&#123;<span class="built_in">round</span>(acc*<span class="number">100</span>)&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(acc_list, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;reverse&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky&#x27;</span>)</span><br><span class="line">plt.plot(acc_list_peeky_rev, <span class="string">&#x27;-o&#x27;</span>, label = <span class="string">&#x27;peeky+reversed&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="../image/nlp-from-scratch/out%207-4.png" alt="out4"></p>
<h2 id="seq2seq을-이용하는-애플리케이션"><a href="#seq2seq을-이용하는-애플리케이션" class="headerlink" title="seq2seq을 이용하는 애플리케이션"></a>seq2seq을 이용하는 애플리케이션</h2><p>seq2seq은 ‘한 시계열 데이터’를 ‘다른 시계열 데이터’로 변환하는 것</p>
<ul>
<li>기계 번역 : ‘한 언어의 문장’을 ‘다른 언어의 문장’으로 변환</li>
<li>자동 요약 : ‘긴 문장’을 ‘짧게 요약한 문장’으로 변환</li>
<li>질의 응답 : ‘질문’을 ‘응답’으로 변환</li>
<li>메일 자동 응답 : ‘받은 메일의 문장’을 ‘답변 글’로 변환</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/nlp/" rel="tag"># nlp</a>
              <a href="/tags/seq2seq/" rel="tag"># seq2seq</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/24/nlp/from-scratch/ch06_LSTM/" rel="prev" title="Ch 06. LSTM">
      <i class="fa fa-chevron-left"></i> Ch 06. LSTM
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/03/nlp/from-scratch/ch08_Attention/" rel="next" title="Ch 08. Attention">
      Ch 08. Attention <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Rnnlm-Generator"><span class="nav-number">1.</span> <span class="nav-text">Rnnlm Generator</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EA%B5%AC%ED%98%84"><span class="nav-number">1.0.1.</span> <span class="nav-text">구현</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%ED%95%99%EC%8A%B5"><span class="nav-number">1.0.2.</span> <span class="nav-text">학습</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EA%B2%B0%EA%B3%BC"><span class="nav-number">1.0.3.</span> <span class="nav-text">결과</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6%EC%9E%A5%EC%97%90%EC%84%9C-%ED%95%99%EC%8A%B5%EC%8B%9C%EC%BC%B0%EB%8D%98-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%B4-%EB%AC%B8%EC%9E%A5-%EC%83%9D%EC%84%B1"><span class="nav-number">1.0.4.</span> <span class="nav-text">6장에서 학습시켰던 모델을 사용해 문장 생성</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EA%B2%B0%EA%B3%BC-1"><span class="nav-number">1.0.5.</span> <span class="nav-text">결과</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Better-RnnlmGen"><span class="nav-number">1.1.</span> <span class="nav-text">Better RnnlmGen</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2seq"><span class="nav-number">2.</span> <span class="nav-text">Seq2seq</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#toy-problem"><span class="nav-number">2.1.</span> <span class="nav-text">toy problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">2.2.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">2.3.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seq2seq-1"><span class="nav-number">2.4.</span> <span class="nav-text">Seq2seq</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EB%8D%A7%EC%85%88-%EB%AC%B8%EC%A0%9C"><span class="nav-number">2.5.</span> <span class="nav-text">덧셈 문제</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Seq2seq-Evaluation-%EA%B5%AC%ED%98%84"><span class="nav-number">2.5.1.</span> <span class="nav-text">Seq2seq Evaluation 구현</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0"><span class="nav-number">2.5.2.</span> <span class="nav-text">데이터 불러오기</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2seq-%EA%B0%9C%EC%84%A0"><span class="nav-number">3.</span> <span class="nav-text">seq2seq 개선</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reverse"><span class="nav-number">3.1.</span> <span class="nav-text">Reverse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Peeky-Decoder"><span class="nav-number">3.2.</span> <span class="nav-text">Peeky Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Peeky-seq2seq-%ED%95%99%EC%8A%B5"><span class="nav-number">3.3.</span> <span class="nav-text">Peeky seq2seq 학습</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Peeky-seq2seq-reversed-%ED%95%99%EC%8A%B5"><span class="nav-number">3.4.</span> <span class="nav-text">Peeky seq2seq(+ reversed) 학습</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2seq%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EB%8A%94-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98"><span class="nav-number">4.</span> <span class="nav-text">seq2seq을 이용하는 애플리케이션</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dongyoung Kim</p>
  <div class="site-description" itemprop="description">Mathematics, Python, ML, Data</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dongyoung0" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dongyoung0" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wmkimdy@gmail.com" title="E-Mail → mailto:wmkimdy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongyoung Kim</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
