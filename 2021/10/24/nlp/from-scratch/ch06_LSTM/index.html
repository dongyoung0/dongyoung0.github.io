<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="기존 RNN의 문제점 시계열 데이터에서 멀리 떨어진, 장기(long term) 의존 관계를 잘 학습할 수 없음  BPTT에서 기울기 소실&#x2F;폭발이 일어나기 때문     ‘?’에 들어가는 단어는 ‘Tom’RNNLM이 위 질문에 답하기 위해서는 ‘Tom이 방에서 TV를 보고 있음’과 ‘Mary가 방에 들어옴’이라는 두 정보를 모두 기억해야 함.위 그림에서 빨간색">
<meta property="og:type" content="article">
<meta property="og:title" content="Ch 06. LSTM">
<meta property="og:url" content="http://example.com/2021/10/24/nlp/from-scratch/ch06_LSTM/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="기존 RNN의 문제점 시계열 데이터에서 멀리 떨어진, 장기(long term) 의존 관계를 잘 학습할 수 없음  BPTT에서 기울기 소실&#x2F;폭발이 일어나기 때문     ‘?’에 들어가는 단어는 ‘Tom’RNNLM이 위 질문에 답하기 위해서는 ‘Tom이 방에서 TV를 보고 있음’과 ‘Mary가 방에 들어옴’이라는 두 정보를 모두 기억해야 함.위 그림에서 빨간색">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-3.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-4.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-15.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-16.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-17.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-18.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-19.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/out%206-1.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-34.png">
<meta property="og:image" content="http://example.com/image/nlp-from-scratch/fig%206-35.png">
<meta property="article:published_time" content="2021-10-23T15:00:00.000Z">
<meta property="article:modified_time" content="2021-11-08T13:43:46.755Z">
<meta property="article:author" content="Dongyoung Kim">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="LSTM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/nlp-from-scratch/fig%206-3.png">

<link rel="canonical" href="http://example.com/2021/10/24/nlp/from-scratch/ch06_LSTM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Ch 06. LSTM | Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">공부한 내용을 정리하는 장소</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">2</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">7</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/24/nlp/from-scratch/ch06_LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongyoung Kim">
      <meta itemprop="description" content="Mathematics, Python, ML, Data">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ch 06. LSTM<a href="https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name_posts/nlp/from-scratch/ch06_LSTM.md" class="post-edit-link" title="Edit this post" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-24 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-24T00:00:00+09:00">2021-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-08 22:43:46" itemprop="dateModified" datetime="2021-11-08T22:43:46+09:00">2021-11-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/from-scratch/" itemprop="url" rel="index"><span itemprop="name">from-scratch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="기존-RNN의-문제점"><a href="#기존-RNN의-문제점" class="headerlink" title="기존 RNN의 문제점"></a>기존 RNN의 문제점</h2><ul>
<li><p>시계열 데이터에서 멀리 떨어진, 장기(long term) 의존 관계를 잘 학습할 수 없음</p>
<ul>
<li><p>BPTT에서 기울기 소실/폭발이 일어나기 때문</p>
<p><img src="/image/nlp-from-scratch/fig%206-3.png" alt="6-3"></p>
</li>
</ul>
</li>
<li><p>‘?’에 들어가는 단어는 ‘Tom’<br>RNNLM이 위 질문에 답하기 위해서는 ‘Tom이 방에서 TV를 보고 있음’과 ‘Mary가 방에 들어옴’이라는 두 정보를 모두 기억해야 함.<br><img src="/image/nlp-from-scratch/fig%206-4.png" alt="6-4"><br>위 그림에서 빨간색 화살표를 따라 기울기를 전달하여 학습하는데, 단어간의 거리가 멀어지면(long-term) 결국 기울기가 작아지거나(소실), 커져서(폭발) 장기 의존 관계를 학습할 수 없게 됨.  </p>
</li>
<li><p>기울기 소실(vanishing gradient)이 일어나는 이유</p>
<ul>
<li>$y = tanh(x)$의 미분은 ${\partial y \over \partial x} = 1 - y^2$으로 역전파 과정에서 기울기가 tanh 노드를 지날때마다 작아짐</li>
</ul>
</li>
<li><p>기울기 폭발(exploding gradients)</p>
<ul>
<li>Matrix multiplication을 실행하면서 시간에 따라 </li>
<li>Wh의 singular value의 최댓값이 1보다 큰 경우 : 기울기가 exponentially 증가</li>
<li>1보다 작은 경우: 기울기가 exponentially 감소(이 경우 vanishing gradient)<br>[30] ‘On the difficulty of training recurrent neural networks’</li>
</ul>
</li>
</ul>
<h3 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h3><ul>
<li><p>기울기 폭발의 해결방법 중 하나.  </p>
<script type="math/tex; mode=display">if~~\| \hat g \| \geq theshold~:~~~ \hat g = {threshold \over \| \hat g \|} \hat g</script></li>
<li><p>기울기 g가 일정 값(threshold) 이상으로 커지면 기울기를 수정해 주는 것.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_grads</span>(<span class="params">grads, max_norm</span>):</span></span><br><span class="line">    total_norm = <span class="number">0</span></span><br><span class="line">    <span class="comment"># g hat 계산</span></span><br><span class="line">    <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">        total_norm += np.<span class="built_in">sum</span>(grad**<span class="number">2</span>)</span><br><span class="line">    total_norm = np.sqrt(total_norm)</span><br><span class="line"></span><br><span class="line">    rate = max_norm / (total_norm + <span class="number">1e-6</span>)</span><br><span class="line">    <span class="keyword">if</span> rate &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">            grad *= rate</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dW1 = np.random.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">10</span></span><br><span class="line">dW2 = np.random.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">10</span></span><br><span class="line">grads = [dW1, dW2]</span><br><span class="line">max_norm = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(grads)</span><br><span class="line">clip_grads(grads, max_norm)</span><br><span class="line"><span class="built_in">print</span>(grads)</span><br></pre></td></tr></table></figure>
<p>기울기가 일정 값(이 경우 5)를 넘어가면 잘 수정해주는 것을 볼 수 있음</p>
<pre><code>[array([[6.49144048, 2.78487283, 6.76254902],
       [5.90862817, 0.23981882, 5.58854088],
       [2.59252447, 4.15101197, 2.83525082]]), array([[6.93137918, 4.40453718, 1.56867738],
       [5.44649018, 7.80314765, 3.06363532],
       [2.21957884, 3.87971258, 9.3638365 ]])]
[array([[1.49503731, 0.64138134, 1.55747605],
       [1.36081038, 0.05523244, 1.28709139],
       [0.59708178, 0.95601551, 0.65298384]]), array([[1.59635916, 1.01440465, 0.36128056],
       [1.25437583, 1.79713531, 0.70558286],
       [0.51118903, 0.89353281, 2.15657603]])]
</code></pre><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>이제 vanishing gradient를 해결하기 위해서 ‘게이트가 추가된 RNN’을 도입할 것<br>대표적으로 LSTM, GRU가 있고, LSTM을 먼저 다룰 것임  </p>
<p><img src="/image/nlp-from-scratch/fig%206-15.png" alt="6-15"></p>
<ul>
<li>LSTM에는 기존 RNN과 다르게 기억 셀(memory cell) $c$ 라는 경로가 있음  </li>
<li><script type="math/tex">c_t</script> 에는 시각 $t$에서의 LSTM의 기억이 저장되어 있음<br>이 `$c_t$를 통해 $h_t=tanh(c_t)$를 계산  <h4 id="output-gate의-열림-상태"><a href="#output-gate의-열림-상태" class="headerlink" title="output gate의 열림 상태"></a>output gate의 열림 상태</h4></li>
</ul>
<script type="math/tex; mode=display">o = \sigma(x_{t} W_{x}^{(o)} + h_{t-1}W_{h}^{(o)} + b^{(o)})</script><ul>
<li>$tanh(c_t)$의 원소들에 대해 ‘그것이 다음 시각의 hidden state에 얼마나 중요한가’를 조정  <ul>
<li>여기서 $\sigma$(sigmoid)의 출력은 0~1로 데이터를 얼마만큼 통과시킬지를 정하는 비율  </li>
</ul>
</li>
</ul>
<h4 id="output-gate"><a href="#output-gate" class="headerlink" title="output gate"></a>output gate</h4><script type="math/tex; mode=display">h_{t} = o \circ tanh(c_{t})</script><p>여기서 tanh의 출력은 -1.0~1.0으로 ‘정보’의 강약(정도)를 표시한다고 볼 수 있음  </p>
<ul>
<li>정보를 얼마만큼 통과시킬지 정하는 sigmoid와 다르게 실질적인 ‘정보’를 지니는 데이터에는 tanh를 주로 사용</li>
</ul>
<h4 id="forget-gate"><a href="#forget-gate" class="headerlink" title="forget gate"></a>forget gate</h4><p><img src="/image/nlp-from-scratch/fig%206-16.png" alt="6-16"></p>
<ul>
<li><p>$c_{t-1}$의 기억 중 불필요한 기억을 잊게 해주는 게이트  </p>
<script type="math/tex; mode=display">f = \sigma(x_{t} W_{x}^{(f)} + h_{t-1}W_{h}^{(f)} + b^{(f)})</script><script type="math/tex; mode=display">c_{t} = f \circ c_{t-1}</script></li>
</ul>
<h4 id="새로운-기억-셀"><a href="#새로운-기억-셀" class="headerlink" title="새로운 기억 셀"></a>새로운 기억 셀</h4><p><img src="/image/nlp-from-scratch/fig%206-17.png" alt="6-17"></p>
<ul>
<li><p>새로 기억해야 할 정보를 기억 셀 $c$에 추가    </p>
<script type="math/tex; mode=display">g = \tanh(x_{t} W_{x}^{(g)} + h_{t-1}W_{h}^{(g)} + b^{(g)})</script></li>
<li><p>잊어버릴 정보가 아니라 실질적으로 ‘기억’해야할 정보기 때문에 $sigmoid$가 아닌 $tanh$ 사용</p>
<h4 id="input-gate"><a href="#input-gate" class="headerlink" title="input gate"></a>input gate</h4><p><img src="/image/nlp-from-scratch/fig%206-18.png" alt="6-18">   </p>
</li>
<li><p>위의 $g$로 새로 추가되는 정보가 얼마나 가치있는 정보인지 판단하기 위해 input gate로 가중치를 줌</p>
<script type="math/tex; mode=display">i = \sigma(x_{t} W_{x}^{(i)} + h_{t-1}W_{h}^{(i)} + b^{(i)})</script></li>
<li><p>이후 $i$와 $g$의 원소별 곱을 기억 셀 $c$에 추가</p>
</li>
</ul>
<h3 id="LSTM의-기울기-흐름"><a href="#LSTM의-기울기-흐름" class="headerlink" title="LSTM의 기울기 흐름"></a>LSTM의 기울기 흐름</h3><ul>
<li>기억 셀 c의 역전파 흐름을 보면 $+$와 $\times$ 노드를 지남  </li>
<li>$+$ 노드를 지날때는 기울기 변화가 일어나지 않음</li>
<li>$\times$ 노드의 경우 matrix multiplication이 아닌 hadamard product(elementwise)를 계산하기 때문에 기울기 소실이 일어나기 어려움</li>
<li>특히 $\times$ 연산을 제어하는 forget gate가 잊어야할 정보와 잊어서는 안될 정보를 판단해주기 때문에 ‘오래 기억해야 할 정보’의 경우 기울기가 소실 없이 전파될 것을 기대할 수 있음</li>
</ul>
<p><img src="/image/nlp-from-scratch/fig%206-19.png" alt="6-19"></p>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li>아래 네가지 게이트에서 일어나는 계산들은 $W$만 다를 뿐 같은 계산을 수행함.(Affine transformation)</li>
<li>4개의 가중치 $W$를 하나로 모아서 한번의 affine transformation만 수행할것임.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b</span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, h_prev, c_prev</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, H = h_prev.shape</span><br><span class="line">        </span><br><span class="line">        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># slice</span></span><br><span class="line">        f = A[:, :H]</span><br><span class="line">        g = A[:, H:<span class="number">2</span>*H]</span><br><span class="line">        i = A[:, <span class="number">2</span>*H:<span class="number">3</span>*H]</span><br><span class="line">        o = A[:, <span class="number">3</span>*H:]</span><br><span class="line">        </span><br><span class="line">        f = sigmoid(f)</span><br><span class="line">        g = np.tanh(g)</span><br><span class="line">        i = sigmoid(i)</span><br><span class="line">        o = sigmoid(o)</span><br><span class="line">        </span><br><span class="line">        c_next = f * c_prev + g * i</span><br><span class="line">        h_next = o * np.tanh(c_next)</span><br><span class="line">        </span><br><span class="line">        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)</span><br><span class="line">        <span class="keyword">return</span> h_next, c_next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dh_next, dc_next</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        x, h_prev, c_prev, i, f, g, o, c_next = self.cache</span><br><span class="line">        </span><br><span class="line">        tanh_c_next = np.tanh(c_next)</span><br><span class="line">        <span class="comment"># ?</span></span><br><span class="line">        ds = dc_next + (dh_next * o) * (<span class="number">1</span> - tanh_c_next **<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        dc_prev = ds * f</span><br><span class="line">        df = ds * c_prev</span><br><span class="line">        dg = ds * i</span><br><span class="line">        di = ds * g</span><br><span class="line">        do = dh_next * tanh_c_next</span><br><span class="line">        </span><br><span class="line">        df *= f * (<span class="number">1</span> - f)</span><br><span class="line">        dg *= (<span class="number">1</span> - g**<span class="number">2</span>)</span><br><span class="line">        di *= i * (<span class="number">1</span> - i)</span><br><span class="line">        do *= o * (<span class="number">1</span> - o)</span><br><span class="line">        </span><br><span class="line">        dA = np.hstack((df, dg, di, do))</span><br><span class="line">        </span><br><span class="line">        dWh = np.dot(h_prev.T, dA)</span><br><span class="line">        dWx = np.dot(x.T, dA)</span><br><span class="line">        db = dA.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dWx</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = dWh</span><br><span class="line">        self.grads[<span class="number">2</span>][...] = db</span><br><span class="line">        </span><br><span class="line">        dx = np.dot(dA, Wx.T)</span><br><span class="line">        dh_prev = np.dot(dA, Wh.T)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dx, dh_prev, dc_prev</span><br></pre></td></tr></table></figure>
<h3 id="Time-LSTM"><a href="#Time-LSTM" class="headerlink" title="Time LSTM"></a>Time LSTM</h3><ul>
<li>T개의 시계열 데이터를 한꺼번에 처리하는 T개의 LSTM layer</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeLSTM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, Wx, Wh, b, stateful=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line">        self.h, self.c = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        self.dh = <span class="literal">None</span></span><br><span class="line">        self.stateful = stateful</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, D = xs.shape</span><br><span class="line">        H = Wh.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        self.layers = []</span><br><span class="line">        hs = np.empty((N, T, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.h <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.h = np.zeros((N, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.c <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.c = np.zeros((N, H), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            layer = LSTM(*self.params)</span><br><span class="line">            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)</span><br><span class="line">            hs[:, t, :] = self.h</span><br><span class="line">            </span><br><span class="line">            self.layers.append(layer)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> hs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dhs</span>):</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, H = dhs.shape</span><br><span class="line">        D = Wx.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        dxs = np.empty((N, T, D), dtype = <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        dh, dc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        grads = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(T)):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)</span><br><span class="line">            dxs[:, t, :] = dx</span><br><span class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(layer.grads):</span><br><span class="line">                grads[i] += grad</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> i, grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(grads):</span><br><span class="line">            self.grads[i][...] = grad</span><br><span class="line">        </span><br><span class="line">        self.dh = dh</span><br><span class="line">        <span class="keyword">return</span> dxs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span>(<span class="params">self, h, c=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.h, self.c = h, c</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.h, self.c = <span class="literal">None</span>, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="LSTM을-사용한-RNNLM"><a href="#LSTM을-사용한-RNNLM" class="headerlink" title="LSTM을 사용한 RNNLM"></a>LSTM을 사용한 RNNLM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rnnlm</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size=<span class="number">10000</span>, wordvec_size=<span class="number">100</span>, hidden_size=<span class="number">100</span></span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 가중치 초기화</span></span><br><span class="line">        embed_W = (rn(V, D) / <span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx = (rn(D, <span class="number">4</span> * H) / np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_W = (rn(H, V) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 계층 생성</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeAffine(affine_W, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.lstm_layer = self.layers[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 모든 가중치와 기울기를 리스트에 모은다.</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        <span class="keyword">return</span> xs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts</span>):</span></span><br><span class="line">        score = self.predict(xs)</span><br><span class="line">        loss = self.loss_layer.forward(score, ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.lstm_layer.reset_state()</span><br></pre></td></tr></table></figure>
<h3 id="학습"><a href="#학습" class="headerlink" title="학습"></a>학습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> common.optimizer <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> common.trainer <span class="keyword">import</span> RnnlmTrainer</span><br><span class="line"><span class="keyword">from</span> common.util <span class="keyword">import</span> eval_perplexity</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">wordvec_size = <span class="number">100</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">time_size = <span class="number">35</span></span><br><span class="line">lr = <span class="number">20.0</span></span><br><span class="line">max_epoch = <span class="number">4</span></span><br><span class="line">max_grad = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load dataa</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">corpus_test, _, _ = ptb.load_data(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_to_id)</span><br><span class="line">xs = corpus[:-<span class="number">1</span>]</span><br><span class="line">ts = corpus[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model 생성</span></span><br><span class="line">model = Rnnlm(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = SGD(lr)</span><br><span class="line">trainer = RnnlmTrainer(model, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습</span></span><br><span class="line">trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval = <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>| current_epoch 1 |  iters 1 / 1327 | time 1.0497894287109375[s] | perplexity 9999.937519272651
| current_epoch 2 |  iters 1 / 1327 | time 976.033093214035[s] | perplexity 223.09635976041903
| current_epoch 3 |  iters 1 / 1327 | time 1924.3960175514221[s] | perplexity 162.90021814658576
| current_epoch 4 |  iters 1 / 1327 | time 2846.238212585449[s] | perplexity 134.39349578293525
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(trainer.ppl_list[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/image/nlp-from-scratch/out%206-1.png" alt="out6-2"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.reset_state()</span><br><span class="line">ppl_test = eval_perplexity(model, corpus_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test perplexity&#x27;</span>, ppl_test)</span><br></pre></td></tr></table></figure>
<pre><code>test perplexity 135.6399667974982
</code></pre><h2 id="RNNLM-개선"><a href="#RNNLM-개선" class="headerlink" title="RNNLM 개선"></a>RNNLM 개선</h2><ul>
<li>위의 RNNLM을 크게 3가지 방법을 통해 개선할 예정.<ol>
<li>LSTM 계층의 다층화</li>
<li>dropout</li>
<li>가중치 공유</li>
</ol>
</li>
</ul>
<h3 id="LSTM-계층의-다층화"><a href="#LSTM-계층의-다층화" class="headerlink" title="LSTM 계층의 다층화"></a>LSTM 계층의 다층화</h3><ul>
<li>LSTM layer를 여러 겹 쌓아서 모델의 정확도가 향상될 것을 기대.<ul>
<li>PTB dataset의 언어 모델에서는 보통 2~4층에서 좋은 결과를 얻었음</li>
</ul>
</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li>Dropout을 통해 overfitting 방지<ul>
<li>이떄 시계열 방향으로 dropout을 넣으면 시간의 흐름에 따라 정보가 사라질 수 있음</li>
<li>대신 layer 깊이 방향으로 dropout을 적용</li>
</ul>
</li>
<li>Variational Dropout : 같은 layer에 속한 dropout들끼리 mask를 공유하여 dropout을 시간 방향으로 적용.<br><img src="/image/nlp-from-scratch/fig%206-34.png" alt="6-34"></li>
</ul>
<h3 id="가중치-공유-Weight-tying"><a href="#가중치-공유-Weight-tying" class="headerlink" title="가중치 공유(Weight tying)"></a>가중치 공유(Weight tying)</h3><ul>
<li>Embedding layer와 Affine layer가 가중치를 공유함으로써 학습해야하는 parameter 수가 크게 줄어들고(계산량 감소 및 overfitting도 방지), 정확도도 향상<br><img src="/image/nlp-from-scratch/fig%206-35.png" alt="6-35"></li>
</ul>
<h3 id="구현-1"><a href="#구현-1" class="headerlink" title="구현"></a>구현</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BetterRnnlm</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size=<span class="number">10000</span>, wordvec_size=<span class="number">650</span>, hidden_size=<span class="number">650</span>, dropout_ratio=<span class="number">0.5</span></span>):</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line">        </span><br><span class="line">        embed_W = (rn(V, D)/<span class="number">100</span>).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx1 = (rn(D, <span class="number">4</span> * H) / np.sqrt(D)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh1 = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b1 = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wx2 = (rn(D, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_Wh2 = (rn(H, <span class="number">4</span> * H) / np.sqrt(H)).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        lstm_b2 = np.zeros(<span class="number">4</span> * H).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeDropout(dropout_ratio),</span><br><span class="line">            TimeAffine(embed_W.T, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.lstm_layers = [self.layers[<span class="number">2</span>], self.layers[<span class="number">4</span>]]</span><br><span class="line">        self.drop_layers = [self.layers[<span class="number">1</span>], self.layers[<span class="number">3</span>], self.layers[<span class="number">5</span>]]</span><br><span class="line">        </span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, xs, train_flg=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.drop_layers:</span><br><span class="line">            layer.train_fig = train_fig</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        <span class="keyword">return</span> xs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xs, ts,  ustrain_flg=<span class="literal">True</span></span>):</span></span><br><span class="line">        score = self.predict(xs, train_flg)</span><br><span class="line">        loss = self.loss_layer.forward(score, ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.lstm_layers:</span><br><span class="line">            layer.reset_state()        </span><br></pre></td></tr></table></figure>
<h3 id="학습-1"><a href="#학습-1" class="headerlink" title="학습"></a>학습</h3><p>위 Rnnlm과 같은 조건으로 학습(epoch는 4번에서 40번으로 늘려서)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)</span><br><span class="line">optimizer = SGD(lr)</span><br><span class="line">trainer = RnnlmTrainer(model, optimizer)</span><br><span class="line"></span><br><span class="line">best_ppl = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">  trainer.fit(xs, ts, max_epoch=<span class="number">1</span>, batch_size=batch_size, time_size=time_size, max_grad=max_grad)</span><br><span class="line">  model.reset_state()</span><br><span class="line">  ppl = eval_perplexity(model, corpus_val)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;perplexity : <span class="subst">&#123;ppl&#125;</span>&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> best_ppl &gt; ppl:</span><br><span class="line">    best_ppl = ppl</span><br><span class="line">    model.save_params()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    lr /= <span class="number">4.0</span></span><br><span class="line">    optimizer.lr = lr</span><br><span class="line">  </span><br><span class="line">  model.reset_state()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h4 id="출력"><a href="#출력" class="headerlink" title="출력"></a>출력</h4><pre><code>| current_epoch 1 |  iters 1 / 1327 | time 0.4454360008239746[s] | perplexity 10000.160734654946
test perplexity : 199.91582600396907
| current_epoch 2 |  iters 1 / 1327 | time 0.42688775062561035[s] | perplexity 285.35975929723134
test perplexity : 146.85835254402008
| current_epoch 3 |  iters 1 / 1327 | time 0.401918888092041[s] | perplexity 220.99791134777234
test perplexity : 125.61738284895596
| current_epoch 4 |  iters 1 / 1327 | time 0.4348595142364502[s] | perplexity 193.85762995444483
test perplexity : 112.8790201600887
| current_epoch 5 |  iters 1 / 1327 | time 0.41679954528808594[s] | perplexity 171.20541935259646
test perplexity : 105.05696537135755
| current_epoch 6 |  iters 1 / 1327 | time 0.42535948753356934[s] | perplexity 158.62854755201397
test perplexity : 100.56030860396797
| current_epoch 7 |  iters 1 / 1327 | time 0.4258577823638916[s] | perplexity 146.75250942965903
test perplexity : 96.68474006387768
| current_epoch 8 |  iters 1 / 1327 | time 0.42501020431518555[s] | perplexity 141.41520295810776
test perplexity : 94.68295145752191
...
</code></pre><h4 id="epoch-40번-진행-후-평가"><a href="#epoch-40번-진행-후-평가" class="headerlink" title="epoch 40번 진행 후 평가"></a>epoch 40번 진행 후 평가</h4><pre><code>test perplexity : 76.2854392816185
</code></pre>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/nlp/" rel="tag"># nlp</a>
              <a href="/tags/LSTM/" rel="tag"># LSTM</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/23/nlp/from-scratch/ch05_RNN/" rel="prev" title="Ch 05. RNN">
      <i class="fa fa-chevron-left"></i> Ch 05. RNN
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/28/nlp/from-scratch/ch07_seq2seq/" rel="next" title="Ch 07. seq2seq">
      Ch 07. seq2seq <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%EA%B8%B0%EC%A1%B4-RNN%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90"><span class="nav-number">1.</span> <span class="nav-text">기존 RNN의 문제점</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-clipping"><span class="nav-number">1.1.</span> <span class="nav-text">gradient clipping</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">2.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#output-gate%EC%9D%98-%EC%97%B4%EB%A6%BC-%EC%83%81%ED%83%9C"><span class="nav-number">2.0.1.</span> <span class="nav-text">output gate의 열림 상태</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#output-gate"><span class="nav-number">2.0.2.</span> <span class="nav-text">output gate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#forget-gate"><span class="nav-number">2.0.3.</span> <span class="nav-text">forget gate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EC%83%88%EB%A1%9C%EC%9A%B4-%EA%B8%B0%EC%96%B5-%EC%85%80"><span class="nav-number">2.0.4.</span> <span class="nav-text">새로운 기억 셀</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#input-gate"><span class="nav-number">2.0.5.</span> <span class="nav-text">input gate</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM%EC%9D%98-%EA%B8%B0%EC%9A%B8%EA%B8%B0-%ED%9D%90%EB%A6%84"><span class="nav-number">2.1.</span> <span class="nav-text">LSTM의 기울기 흐름</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%EA%B5%AC%ED%98%84"><span class="nav-number">3.</span> <span class="nav-text">구현</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Time-LSTM"><span class="nav-number">3.1.</span> <span class="nav-text">Time LSTM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-RNNLM"><span class="nav-number">4.</span> <span class="nav-text">LSTM을 사용한 RNNLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%ED%95%99%EC%8A%B5"><span class="nav-number">4.1.</span> <span class="nav-text">학습</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNNLM-%EA%B0%9C%EC%84%A0"><span class="nav-number">5.</span> <span class="nav-text">RNNLM 개선</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM-%EA%B3%84%EC%B8%B5%EC%9D%98-%EB%8B%A4%EC%B8%B5%ED%99%94"><span class="nav-number">5.1.</span> <span class="nav-text">LSTM 계층의 다층화</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">5.2.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EA%B0%80%EC%A4%91%EC%B9%98-%EA%B3%B5%EC%9C%A0-Weight-tying"><span class="nav-number">5.3.</span> <span class="nav-text">가중치 공유(Weight tying)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EA%B5%AC%ED%98%84-1"><span class="nav-number">5.4.</span> <span class="nav-text">구현</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%ED%95%99%EC%8A%B5-1"><span class="nav-number">5.5.</span> <span class="nav-text">학습</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EC%B6%9C%EB%A0%A5"><span class="nav-number">5.5.1.</span> <span class="nav-text">출력</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#epoch-40%EB%B2%88-%EC%A7%84%ED%96%89-%ED%9B%84-%ED%8F%89%EA%B0%80"><span class="nav-number">5.5.2.</span> <span class="nav-text">epoch 40번 진행 후 평가</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dongyoung Kim</p>
  <div class="site-description" itemprop="description">Mathematics, Python, ML, Data</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dongyoung0" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dongyoung0" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wmkimdy@gmail.com" title="E-Mail → mailto:wmkimdy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongyoung Kim</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
